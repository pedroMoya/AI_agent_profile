{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyeCwDW69Zf3"
   },
   "source": [
    "# Demo 2 - Multi-Agent Team Interaction (Agent Collaboration)\n",
    "By: [Lior Gazit](https://github.com/LiorGazit).  \n",
    "Repo: [Agents-Over-The-Weekend](https://github.com/PacktPublishing/Agents-Over-The-Weekend/tree/main/Lior_Gazit/workshop_september_2025/)   \n",
    "Running LLMs locally for free: This code leverages [`LLMPop`](https://pypi.org/project/llmpop/) that is dedicated to spinning up local or remote LLMs in a unified and modular syntax.  \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PacktPublishing/Agents-Over-The-Weekend/blob/main/Lior_Gazit/workshop_september_2025/codes_for_Lior_Bootcamp_talk_sept2025_demo2.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a> (pick a GPU Colab session for fastest computing)  \n",
    "\n",
    "```\n",
    "Disclaimer: The content and ideas presented in this notebook are solely those of the author, Lior Gazit, and do not represent the views or intellectual property of the author's employer.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn9qIRLS9Zf7"
   },
   "source": [
    "Installing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NMpPyY829Zf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install llmpop\n",
    "%pip -q install sentence-transformers faiss-cpu langchain tiktoken langsmith langchain_openai -U \"autogen-agentchat\" \"autogen-ext[openai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uU75SUy9ZgA"
   },
   "source": [
    "**Imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MVDmCWnO9ZgB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dr018PW9ZgH",
    "outputId": "b27b338a-18a4-417f-8400-f8d10b8db9f6"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Ollama server at http://127.0.0.1:11434 failed to start within 15s.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllmpop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_llm\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[0;32m----> 4\u001b[0m coder \u001b[38;5;241m=\u001b[39m \u001b[43minit_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCodeLlama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mollama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m reviewer \u001b[38;5;241m=\u001b[39m init_llm(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.2:1b\u001b[39m\u001b[38;5;124m\"\u001b[39m, provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a Python function to check if a number is prime.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/d/sandbox_workshop/.venv/lib/python3.10/site-packages/llmpop/init_llm.py:153\u001b[0m, in \u001b[0;36minit_llm\u001b[0;34m(model, provider, provider_kwargs, verbose, **chat_init_kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⏳ Waiting for Ollama to be ready…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m \u001b[43mwait_for_ollama_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReady!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/d/sandbox_workshop/.venv/lib/python3.10/site-packages/llmpop/init_llm.py:24\u001b[0m, in \u001b[0;36mwait_for_ollama_ready\u001b[0;34m(host, port, timeout)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError:\n\u001b[1;32m     23\u001b[0m         sleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama server at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed to start within \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Ollama server at http://127.0.0.1:11434 failed to start within 15s."
     ]
    }
   ],
   "source": [
    "from llmpop import init_llm\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "coder = init_llm(model=\"CodeLlama\", provider=\"ollama\", verbose=False)\n",
    "reviewer = init_llm(model=\"llama3.2:1b\", provider=\"ollama\", verbose=False)\n",
    "\n",
    "task = \"Write a Python function to check if a number is prime.\"\n",
    "\n",
    "# ---- Agent A: Coder ----\n",
    "coder_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "System: You are a coding agent. Output *code only*.\n",
    "User task: {task}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "coder_result = (coder_prompt | coder).invoke({\"task\": task})\n",
    "coder_code = getattr(coder_result, \"content\", str(coder_result)).strip()\n",
    "\n",
    "# ---- Agent B: Reviewer ----\n",
    "reviewer_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "System: You are a strict code reviewer for Python.\n",
    "Given the code below, evaluate for:\n",
    "- Correctness and edge cases\n",
    "- Time complexity reasonableness\n",
    "- Readability (within reason, given constraints)\n",
    "If changes are needed, provide a brief rationale followed by a *fully corrected* version.\n",
    "If it's good as-is, say \"APPROVED\" and explain briefly why.\n",
    "\n",
    "Code:\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "review_result = (reviewer_prompt | reviewer).invoke({\"code\": coder_code})\n",
    "review_text = getattr(review_result, \"content\", str(review_result)).strip()\n",
    "\n",
    "print(\"\\n=== Coder output ===\\n\")\n",
    "print(coder_code)\n",
    "\n",
    "print(\"\\n=== Reviewer feedback ===\\n\")\n",
    "print(review_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P4Ej8DS9ZgI"
   },
   "source": [
    "Now, here is an example using AutoGen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CtutwBfT9ZgJ",
    "outputId": "e487da00-d551-42e1-d00d-83b9767ca71a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "# In Colab, use getpass to securely prompt for your API key\n",
    "from getpass import getpass\n",
    "import openai\n",
    "\n",
    "openai.api_key = getpass(\"Paste your OpenAI API key: \")\n",
    "\n",
    "# 1. Import the agent classes and the OpenAI client\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "async def multi_agent_demo():\n",
    "    # 2. Configure your OpenAI API key\n",
    "    api_key = openai.api_key\n",
    "    if not api_key:\n",
    "        openai.api_key = getpass(\"Paste your OpenAI API key: \")\n",
    "\n",
    "    # 3. Create the OpenAI model client\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=api_key,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    # 4. Instantiate two LLM agents with distinct roles\n",
    "    coder = AssistantAgent(\n",
    "        name=\"Coder\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"You are a Python coding assistant. Produce only working code.\"\n",
    "    )\n",
    "    reviewer = AssistantAgent(\n",
    "        name=\"Reviewer\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"You are a code reviewer. Point out bugs or edge cases.\"\n",
    "    )\n",
    "\n",
    "    # 5. Coder agent writes a function\n",
    "    code_task = \"Write a Python function `is_prime(n)` that returns True if `n` is prime.\"\n",
    "    code = await coder.run(task=code_task)\n",
    "    print(\"=== Coder’s Output ===\\n\")\n",
    "    for msg in code.messages:\n",
    "        print(msg.content)\n",
    "\n",
    "    # 6. Reviewer agent critiques the code\n",
    "    review = await reviewer.run(task=f\"Review the following code for correctness and edge cases:\\n\\n{code}\")\n",
    "    print(\"\\n=== Reviewer’s Feedback ===\\n\")\n",
    "    for msg in review.messages:\n",
    "        print(msg.content)\n",
    "\n",
    "    # 7. Clean up\n",
    "    await model_client.close()\n",
    "\n",
    "# 8. Execute the multi‑agent demo\n",
    "await multi_agent_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCIX_C-MVGEt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (sandbox_workshop)",
   "language": "python",
   "name": "sandbox_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
