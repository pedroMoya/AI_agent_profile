{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyeCwDW69Zf3"
   },
   "source": [
    "# Demo 4 - Developing a LLM Router with `LLMPop` Using GPT5 for Vibe Coding\n",
    "By: [Lior Gazit](https://github.com/LiorGazit).  \n",
    "Repo: [Agents-Over-The-Weekend](https://github.com/PacktPublishing/Agents-Over-The-Weekend/tree/main/Lior_Gazit/workshop_september_2025/)  \n",
    "Running LLMs locally for free: This code leverages [`LLMPop`](https://pypi.org/project/llmpop/) that is dedicated to spinning up local or remote LLMs in a unified and modular syntax.    \n",
    "\n",
    "In this notebook we experiment vibe-coding with ChatGPT.  \n",
    "We do so by providing GPT5 with a single prompt + reference material so to design a smart LLM router that maps each user prompt to the optimal LLM based on considerations of cost, speed, and depth.  \n",
    "Reference material provided to GPT5 for building the code solution:    \n",
    "1. A machine-readable guide for the library `LLMPop`, a library that unifies call syntax to all LLMs, built to suit LLM routers.  \n",
    "2. A table of LLMs so to provide GPT5 context on various LLMs so to build the routing logic around.  \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PacktPublishing/Agents-Over-The-Weekend/blob/main/Lior_Gazit/workshop_september_2025/codes_for_Lior_Bootcamp_talk_sept2025_demo4.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a> (pick a GPU Colab session for fastest computing)  \n",
    "\n",
    "```\n",
    "Disclaimer: The content and ideas presented in this notebook are solely those of the author, Lior Gazit, and do not represent the views or intellectual property of the author's employer.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "948vnVL3fCJU"
   },
   "source": [
    "## Prompt for coder-LLM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfuxtDbzfjQb"
   },
   "source": [
    "\n",
    "Write a Python script (Later to become a stand alone Jupyter notebook) that demonstrates a small-scale LLM router using the LLMPop library.\n",
    "\n",
    "For information on LLMPop:  \n",
    "Details on the LLMPop library are described in the attached document: LLM_READABLE_GUIDE.md  \n",
    "Refere to LLM_READABLE_GUIDE.md for understanding how to use the LLMPop library.  \n",
    "\n",
    "For information on the different models available on Ollama:  \n",
    "Refer to the file Ollama models by size and call name (a limited list).csv  \n",
    "It has a limited list of models, and it gives some idea on which are lighted and which are heavier in size.  \n",
    "\n",
    "### Requirements:\n",
    "\n",
    "1. Purpose: The script routes user prompts to one of several available LLMs (some from Ollama for local inference, others from OpenAI’s API, per the LLMPop libraries functionality) based on simple routing logic.\n",
    "2. Inputs to Routing Function:\n",
    "   * User prompt (text)\n",
    "   * Optional conversation context (previous messages, user metadata, etc.)\n",
    "   * Optional optimization criteria (e.g., prioritize cost, speed, or reasoning depth)\n",
    "   * Optional runtime resource information (e.g., system load, API quotas)\n",
    "3. Routing Logic: Implement a basic, illustrative strategy to decide which model to call.  \n",
    "   For example:\n",
    "   * Short/simple prompts → lightweight, fast model (local Ollama model).\n",
    "   * Complex prompts (math/logic/code) → reasoning-heavy OpenAI GPT-4 model.\n",
    "   * Requests flagged as “specialized” → a chosen domain-specific model.\n",
    "   * Add dummy evaluation for resource optimization (simulate cost or latency).\n",
    "4. Output: Show which model is selected, call it with a test prompt, and print a sample response.\n",
    "5. Implementation Notes:\n",
    "   * Import and use functions/classes from LLMPop (assume the library is installed and documentation is available).\n",
    "   * Include a main() function that:\n",
    "     * Accepts a sample user query,\n",
    "     * Runs it through your routing logic,\n",
    "     * Dispatches it to the chosen LLM.\n",
    "   * Add inline comments explaining the decision-making process.\n",
    "   * Keep it minimal, clean, and easy to adapt as a demo or template.\n",
    "\n",
    "### Output Expectations:\n",
    "* Full runnable Python code (later to be a notebook).\n",
    "* Clear, commented code that demonstrates basic extensibility for future routing improvements.\n",
    "* Use resource checks.\n",
    "\n",
    "### Goal:\n",
    "Deliver a concise, readable prototype demonstrating LLM routing patterns, not a production service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Akit5rDfOtR"
   },
   "source": [
    "## The perliminary code the bot wrote:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idfDz4Bufnzp"
   },
   "source": [
    "```python\n",
    "\n",
    "\"\"\"\n",
    "Small-Scale LLM Router Demo\n",
    "---------------------------\n",
    "Purpose:\n",
    "  Route a user prompt to a local (Ollama) or remote (OpenAI) model using basic,\n",
    "  illustrative heuristics. Keep it readable and easy to adapt.\n",
    "\n",
    "Prereqs:\n",
    "  pip -q install llmpop\n",
    "  pip install langchain_core psutil pandas\n",
    "\n",
    "Notes:\n",
    "  - Reads an optional CSV of Ollama models to choose a lightweight or heavier local model:\n",
    "      /mnt/data/Ollama models by size and call name (a limited list).csv\n",
    "  - Uses `init_llm` from LLMPop (returns a LangChain ChatModel).\n",
    "  - Uses simple heuristics to choose a model: complexity, specialization, optimization,\n",
    "    plus a quick CPU/memory snapshot and dummy cost/latency scoring.\n",
    "  - Falls back gracefully if a model or provider isn't available.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# --- LLMPop imports (per your guide) ---\n",
    "# Guide reference: init_llm() returns a ChatModel (ChatOllama / ChatOpenAI)\n",
    "# and start_resource_monitoring() optionally logs CPU/Mem/GPU in a background thread.\n",
    "# (This demo just shows you how to call them; monitoring is optional.)\n",
    "from llmpop import init_llm, start_resource_monitoring  # :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "# --- Optional utilities ---\n",
    "import psutil\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# Model catalog & metadata\n",
    "# -----------------------\n",
    "\n",
    "@dataclass\n",
    "class ModelSpec:\n",
    "    name: str               # provider-specific model id (e.g., 'llama3:8b' or 'gpt-4o-mini')\n",
    "    provider: str           # 'ollama' or 'openai'\n",
    "    size_rank: int          # smaller number = lighter/faster (heuristic)\n",
    "    tags: List[str]         # ['general', 'code', 'math', 'reasoning', 'specialized:finance', ...]\n",
    "    est_cost_per_1k: float  # dummy USD per 1k tokens (for routing illustration)\n",
    "    est_latency: float      # dummy seconds base latency (for routing illustration)\n",
    "    notes: str = \"\"\n",
    "\n",
    "def _read_ollama_csv(csv_path: str) -> List[ModelSpec]:\n",
    "    \"\"\"\n",
    "    Read the user's 'Ollama models by size and call name (a limited list).csv'\n",
    "    and convert it into ModelSpec entries with rough size ranks.\n",
    "    The CSV is expected to have columns like: 'call_name', 'size' (text), etc.\n",
    "    This function is defensive and will skip rows missing these fields.\n",
    "    \"\"\"\n",
    "    specs: List[ModelSpec] = []\n",
    "    if not os.path.exists(csv_path):\n",
    "        return specs\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception:\n",
    "        return specs\n",
    "\n",
    "    # Try to infer a size rank from a 'size' or 'parameters' column\n",
    "    def infer_size_rank(size_str: str) -> int:\n",
    "        if not isinstance(size_str, str):\n",
    "            return 5\n",
    "        s = size_str.lower()\n",
    "        # simple heuristics: lower rank is \"lighter\"\n",
    "        if any(k in s for k in [\"2b\", \"3b\", \"tiny\", \"mini\", \"small\"]): return 1\n",
    "        if any(k in s for k in [\"7b\", \"8b\", \"medium\"]): return 2\n",
    "        if any(k in s for k in [\"13b\", \"14b\", \"large\"]): return 3\n",
    "        if any(k in s for k in [\"30b\", \"34b\", \"xl\"]): return 4\n",
    "        return 5\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        call_name = str(row.get(\"call_name\") or row.get(\"name\") or \"\").strip()\n",
    "        size_str  = str(row.get(\"size\") or row.get(\"parameters\") or \"\").strip()\n",
    "        if not call_name:\n",
    "            continue\n",
    "        specs.append(\n",
    "            ModelSpec(\n",
    "                name=call_name,\n",
    "                provider=\"ollama\",\n",
    "                size_rank=infer_size_rank(size_str),\n",
    "                tags=[\"general\"],\n",
    "                est_cost_per_1k=0.0,        # local inference → treat as $0 for demo\n",
    "                est_latency=max(0.2, 0.05 * infer_size_rank(size_str)),\n",
    "                notes=f\"From CSV; size='{size_str}'\"\n",
    "            )\n",
    "        )\n",
    "    return specs\n",
    "\n",
    "def _default_catalog() -> List[ModelSpec]:\n",
    "    \"\"\"\n",
    "    Default demo catalog if CSV is unavailable. Adjust to your environment.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        # Local (Ollama) — fast/light general chat\n",
    "        ModelSpec(name=\"llama3.1:8b-instruct\", provider=\"ollama\", size_rank=2,\n",
    "                  tags=[\"general\"], est_cost_per_1k=0.0, est_latency=0.3,\n",
    "                  notes=\"Local, lightweight general model\"),\n",
    "        # Local (Ollama) — code-aware (replace with your local favorite)\n",
    "        ModelSpec(name=\"codellama:7b-instruct\", provider=\"ollama\", size_rank=2,\n",
    "                  tags=[\"code\"], est_cost_per_1k=0.0, est_latency=0.35,\n",
    "                  notes=\"Local, code-oriented\"),\n",
    "        # Remote (OpenAI) — reasoning-heavy\n",
    "        ModelSpec(name=\"gpt-4o\", provider=\"openai\", size_rank=4,\n",
    "                  tags=[\"reasoning\", \"math\", \"complex\"], est_cost_per_1k=5.0, est_latency=0.6,\n",
    "                  notes=\"Remote, strong reasoning\"),\n",
    "        # Remote (OpenAI) — fast/cheap\n",
    "        ModelSpec(name=\"gpt-4o-mini\", provider=\"openai\", size_rank=1,\n",
    "                  tags=[\"general\", \"fast\"], est_cost_per_1k=0.6, est_latency=0.25,\n",
    "                  notes=\"Remote, budget/latency friendly\"),\n",
    "    ]\n",
    "\n",
    "def build_catalog(csv_path: str) -> List[ModelSpec]:\n",
    "    specs = _read_ollama_csv(csv_path)\n",
    "    specs += _default_catalog()\n",
    "    return specs\n",
    "\n",
    "# -----------------------\n",
    "# Lightweight heuristics\n",
    "# -----------------------\n",
    "\n",
    "def is_complex_prompt(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Naive complexity signal: math markers, code fences, long length, or\n",
    "    words indicating step-by-step reasoning.\n",
    "    \"\"\"\n",
    "    if len(text) > 300:\n",
    "        return True\n",
    "    code_markers = [\"```\", \"def \", \"class \", \"SELECT \", \"FROM \", \"import \", \"public static void\"]\n",
    "    math_markers = [r\"\\bprove\\b\", r\"\\bderive\\b\", r\"\\bO\\(\", r\"\\bintegral\\b\", r\"[0-9]+\\s*[\\+\\-\\*\\/\\^]\\s*[0-9]+\"]\n",
    "    reasoning_terms = [r\"\\bchain of thought\\b\", r\"\\bstep-by-step\\b\", r\"\\bthink step by step\\b\"]\n",
    "\n",
    "    if any(m in text for m in code_markers):\n",
    "        return True\n",
    "    if any(re.search(p, text, flags=re.IGNORECASE) for p in math_markers + reasoning_terms):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def detect_specialization(text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Return a specialization tag when obvious (very naive).\n",
    "    \"\"\"\n",
    "    pairs = {\n",
    "        \"code\": [r\"\\bwrite (a|the) (python|js|sql|java|c\\+\\+|c#)\"],\n",
    "        \"sql\":  [r\"\\bSELECT\\b.*\\bFROM\\b\", r\"\\bCREATE TABLE\\b\"],\n",
    "        \"finance\": [r\"\\bDCF\\b\", r\"\\bWACC\\b\", r\"\\b10-K\\b\"],\n",
    "        \"biology\": [r\"\\bRNA\\b\", r\"\\bCRISPR\\b\"],\n",
    "        \"law\": [r\"\\bstatute\\b\", r\"\\btort\\b\"],\n",
    "        \"math\": [r\"\\bprove\\b\", r\"\\bintegral\\b\", r\"\\bderivative\\b\"],\n",
    "    }\n",
    "    for tag, patterns in pairs.items():\n",
    "        if any(re.search(p, text, flags=re.IGNORECASE | re.DOTALL) for p in patterns):\n",
    "            return tag\n",
    "    return None\n",
    "\n",
    "def snapshot_resources() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Quick resource snapshot (CPU %, available RAM GB). These values can inform routing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cpu = psutil.cpu_percent(interval=0.2)\n",
    "        mem = psutil.virtual_memory()\n",
    "        avail_gb = round(mem.available / (1024**3), 2)\n",
    "        return {\"cpu_percent\": cpu, \"available_gb\": avail_gb}\n",
    "    except Exception:\n",
    "        return {\"cpu_percent\": None, \"available_gb\": None}\n",
    "\n",
    "def score_candidate(\n",
    "    spec: ModelSpec,\n",
    "    *,\n",
    "    optimization: str,\n",
    "    complex_prompt: bool,\n",
    "    specialization: Optional[str],\n",
    "    resources: Dict[str, Any]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Lower score = better. Combine a few simple signals:\n",
    "      - If complex → prefer 'reasoning' models.\n",
    "      - If specialization detected → prefer related tags (e.g., 'code', 'math').\n",
    "      - Optimization: 'speed', 'cost', or 'depth' (reasoning depth).\n",
    "      - If CPU is high and provider == 'ollama' → small penalty (local load).\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "\n",
    "    # Base on optimization goal\n",
    "    if optimization == \"speed\":\n",
    "        score += spec.est_latency * 2.0 + spec.size_rank * 0.5\n",
    "    elif optimization == \"cost\":\n",
    "        score += spec.est_cost_per_1k * 1.5 + spec.size_rank * 0.3\n",
    "    elif optimization == \"depth\":\n",
    "        # prefer reasoning-heavy\n",
    "        score += (0 if (\"reasoning\" in spec.tags or \"complex\" in spec.tags) else 1.5)\n",
    "        score += spec.size_rank * 0.2\n",
    "    else:\n",
    "        # default blend\n",
    "        score += spec.est_latency + 0.2 * spec.est_cost_per_1k\n",
    "\n",
    "    # Complexity\n",
    "    if complex_prompt and not any(t in spec.tags for t in [\"reasoning\", \"complex\", \"math\", \"code\"]):\n",
    "        score += 1.0\n",
    "\n",
    "    # Specialization\n",
    "    if specialization:\n",
    "        if specialization in spec.tags or f\"specialized:{specialization}\" in spec.tags:\n",
    "            score -= 0.5  # small bonus\n",
    "        else:\n",
    "            # mild penalty if it doesn't look appropriate\n",
    "            score += 0.3\n",
    "\n",
    "    # Resource-aware: if CPU is busy and we're local, prefer remote\n",
    "    cpu = resources.get(\"cpu_percent\")\n",
    "    if cpu is not None and cpu > 70 and spec.provider == \"ollama\":\n",
    "        score += 0.8\n",
    "\n",
    "    # If RAM is very low and spec is large local model (size_rank>=3), penalize\n",
    "    avail_gb = resources.get(\"available_gb\")\n",
    "    if avail_gb is not None and avail_gb < 2.0 and spec.provider == \"ollama\" and spec.size_rank >= 3:\n",
    "        score += 0.7\n",
    "\n",
    "    return score\n",
    "\n",
    "# -----------------------\n",
    "# Dispatch\n",
    "# -----------------------\n",
    "\n",
    "def init_model(spec: ModelSpec):\n",
    "    \"\"\"\n",
    "    Initialize a ChatModel via LLMPop's init_llm().\n",
    "    For Ollama, auto_serve/auto_install/pull are enabled to make the demo smoother.\n",
    "    For OpenAI, use env OPENAI_API_KEY or pass provider_kwargs with api_key.\n",
    "    \"\"\"\n",
    "    if spec.provider == \"ollama\":\n",
    "        return init_llm(\n",
    "            model=spec.name,\n",
    "            provider=\"ollama\",\n",
    "            provider_kwargs={\n",
    "                \"auto_install\": True,\n",
    "                \"auto_serve\": True,\n",
    "                \"pull\": True,\n",
    "            },\n",
    "            temperature=0.0,\n",
    "        )  # :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "    elif spec.provider == \"openai\":\n",
    "        # You can also pass provider_kwargs={\"api_key\": \"...\"} (guide allows both env or kwarg)\n",
    "        return init_llm(\n",
    "            model=spec.name,\n",
    "            provider=\"openai\",\n",
    "            temperature=0.0,\n",
    "        )  # :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {spec.provider}\")\n",
    "\n",
    "def call_model(chat_model, user_prompt: str, system_msg: str = \"You are a helpful assistant.\"):\n",
    "    \"\"\"\n",
    "    Call a LangChain ChatModel returned by init_llm(). Keep it simple with a short system + human turn.\n",
    "    \"\"\"\n",
    "    from langchain_core.messages import SystemMessage, HumanMessage\n",
    "    # Return raw content; for pure string you could add StrOutputParser (see guide). :contentReference[oaicite:4]{index=4}\n",
    "    result = chat_model.invoke([SystemMessage(content=system_msg), HumanMessage(content=user_prompt)])\n",
    "    return getattr(result, \"content\", str(result))\n",
    "\n",
    "# -----------------------\n",
    "# Main router\n",
    "# -----------------------\n",
    "\n",
    "def route_and_dispatch(\n",
    "    prompt: str,\n",
    "    *,\n",
    "    conversation_ctx: Optional[List[Dict[str, Any]]] = None,\n",
    "    optimization: str = \"speed\",   # 'speed' | 'cost' | 'depth'\n",
    "    runtime_info: Optional[Dict[str, Any]] = None,\n",
    "    csv_path: str = \"/mnt/data/Ollama models by size and call name (a limited list).csv\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Core router: choose a model and execute the call. Returns a dict with\n",
    "    selection details and the model output.\n",
    "    \"\"\"\n",
    "    catalog = build_catalog(csv_path)\n",
    "\n",
    "    complex_prompt = is_complex_prompt(prompt)\n",
    "    specialization = detect_specialization(prompt)\n",
    "    resources = snapshot_resources()\n",
    "\n",
    "    # Incorporate any runtime quotas into a tiny penalty (dummy)\n",
    "    quotas = (runtime_info or {}).get(\"api_quotas\", {})\n",
    "    openai_quota_low = quotas.get(\"openai_remaining\", 1000) < 50\n",
    "    ollama_quota_low = quotas.get(\"ollama_remaining\", 1000) < 10\n",
    "\n",
    "    # Score each candidate\n",
    "    scored = []\n",
    "    for spec in catalog:\n",
    "        s = score_candidate(spec,\n",
    "                            optimization=optimization,\n",
    "                            complex_prompt=complex_prompt,\n",
    "                            specialization=specialization,\n",
    "                            resources=resources)\n",
    "        # Quotas as penalties (illustrative)\n",
    "        if spec.provider == \"openai\" and openai_quota_low:\n",
    "            s += 1.0\n",
    "        if spec.provider == \"ollama\" and ollama_quota_low:\n",
    "            s += 0.5\n",
    "        scored.append((s, spec))\n",
    "\n",
    "    scored.sort(key=lambda x: x[0])\n",
    "    chosen = scored[0][1]\n",
    "\n",
    "    # Try to init and call; fall back if needed\n",
    "    error = None\n",
    "    response = None\n",
    "    try:\n",
    "        chat_model = init_model(chosen)\n",
    "        response = call_model(chat_model, prompt)\n",
    "    except Exception as e:\n",
    "        error = f\"{type(e).__name__}: {e}\"\n",
    "        # Fallback: attempt the next candidate\n",
    "        if len(scored) > 1:\n",
    "            try:\n",
    "                alt = scored[1][1]\n",
    "                chat_model = init_model(alt)\n",
    "                response = call_model(chat_model, prompt)\n",
    "                chosen = alt\n",
    "            except Exception as e2:\n",
    "                response = f\"[Router demo fallback] Could not reach any LLM. Last error: {type(e2).__name__}: {e2}\"\n",
    "\n",
    "    return {\n",
    "        \"chosen_model\": chosen.name,\n",
    "        \"provider\": chosen.provider,\n",
    "        \"notes\": chosen.notes,\n",
    "        \"complex_prompt\": complex_prompt,\n",
    "        \"specialization\": specialization,\n",
    "        \"optimization\": optimization,\n",
    "        \"resources\": resources,\n",
    "        \"error\": error,\n",
    "        \"response\": response,\n",
    "    }\n",
    "\n",
    "# -----------------------\n",
    "# Demo entrypoint\n",
    "# -----------------------\n",
    "\n",
    "def main():\n",
    "    # Optional: start a background resource logger (1 minute, every 5s) — for demo visibility.\n",
    "    # This is purely illustrative; comment out if not desired.\n",
    "    try:\n",
    "        _ = start_resource_monitoring(logfile=\"resource_usage.log\", duration=60, interval=5)  # :contentReference[oaicite:5]{index=5}\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Sample inputs (edit these to experiment)\n",
    "    sample_prompt = (\n",
    "        \"Write a short Python function that returns the nth Fibonacci number, \"\n",
    "        \"then analyze the time complexity.\"\n",
    "    )\n",
    "    conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
    "    optimization = \"depth\"  # try 'speed', 'cost', or 'depth'\n",
    "\n",
    "    # Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
    "    runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
    "\n",
    "    result = route_and_dispatch(\n",
    "        prompt=sample_prompt,\n",
    "        conversation_ctx=conversation_ctx,\n",
    "        optimization=optimization,\n",
    "        runtime_info=runtime_info,\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Router Decision ===\")\n",
    "    print(f\"Chosen: {result['chosen_model']} (provider={result['provider']})\")\n",
    "    print(f\"Notes:  {result['notes']}\")\n",
    "    print(f\"Complex prompt? {result['complex_prompt']}; specialization: {result['specialization']}\")\n",
    "    print(f\"Optimization: {result['optimization']}\")\n",
    "    print(f\"Resources snapshot: {result['resources']}\")\n",
    "    if result[\"error\"]:\n",
    "        print(f\"Init error (first try): {result['error']}\")\n",
    "\n",
    "    print(\"\\n=== Model Output (truncated) ===\")\n",
    "    print(str(result[\"response\"])[:1200])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deKBVzaiJ6ZY"
   },
   "source": [
    "## The Actual Resulting Code\n",
    "I took the code that the bot generated after a single prompt, and adjusted it to suit my needs.  \n",
    "All adjustments were minor.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfNq2s9Ufv57"
   },
   "source": [
    "The bot told about the code it designed:  \n",
    "\n",
    "**Small-Scale LLM Router Demo**  \n",
    "Purpose:  \n",
    "  Route a user prompt to a local (Ollama) or remote (OpenAI) model using basic,\n",
    "  illustrative heuristics. Keep it readable and easy to adapt.  \n",
    "\n",
    "Prereqs:  \n",
    "`pip -q install llmpop `  \n",
    "`pip install langchain_core psutil pandas`  \n",
    "\n",
    "Notes:  \n",
    "  - Reads an optional CSV of Ollama models to choose a lightweight or heavier local model:  \n",
    "      `Ollama models by size and call name (a limited list).csv`  \n",
    "  - Uses `init_llm` from LLMPop (returns a LangChain ChatModel).  \n",
    "  - Uses simple heuristics to choose a model: complexity, specialization, optimization,\n",
    "    plus a quick CPU/memory snapshot and dummy cost/latency scoring.  \n",
    "  - Falls back gracefully if a model or provider isn't available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haMbL9fiK4vY"
   },
   "source": [
    "**Installations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "icke82ozgLsE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install llmpop\n",
    "%pip -q install langchain_core psutil pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEFEfTYRK8-p"
   },
   "source": [
    "**Imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "P2LJUSLogQGy"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "from getpass import getpass\n",
    "\n",
    "# --- LLMPop imports (per LLM_READABLE_GUIDE.md) ---\n",
    "# Guide reference: init_llm() returns a ChatModel (ChatOllama / ChatOpenAI)\n",
    "# and start_resource_monitoring() optionally logs CPU/Mem/GPU in a background thread.\n",
    "from llmpop import init_llm, start_resource_monitoring  # :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "# --- Optional utilities ---\n",
    "import psutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjUMNrMsMGNx"
   },
   "source": [
    "**Settings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rQZi-P0HMG4n"
   },
   "outputs": [],
   "source": [
    "allow_paid_models = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aug3DrkL7u-"
   },
   "source": [
    "**Insert your OpenAI API key:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_3kXhw3JhLK2",
    "outputId": "6e23a3c3-50a0-4626-fe3f-43e5ac987042"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "if allow_paid_models:\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rztR3Endgav6"
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Model catalog & metadata\n",
    "# -----------------------\n",
    "\n",
    "@dataclass\n",
    "class ModelSpec:\n",
    "    name: str               # provider-specific model id (e.g., 'llama3:8b' or 'gpt-4o-mini')\n",
    "    provider: str           # 'ollama' or 'openai'\n",
    "    size_rank: int          # smaller number = lighter/faster (heuristic)\n",
    "    tags: List[str]         # ['general', 'code', 'math', 'reasoning', 'specialized:finance', ...]\n",
    "    est_cost_per_1k: float  # dummy USD per 1k tokens (for routing illustration)\n",
    "    est_latency: float      # dummy seconds base latency (for routing illustration)\n",
    "    notes: str = \"\"\n",
    "\n",
    "def _read_ollama_csv(csv_path: str) -> List[ModelSpec]:\n",
    "    \"\"\"\n",
    "    Read the user's 'Ollama models by size and call name (a limited list).csv'\n",
    "    and convert it into ModelSpec entries with rough size ranks.\n",
    "    The CSV is expected to have columns like: 'call_name', 'size' (text), etc.\n",
    "    This function is defensive and will skip rows missing these fields.\n",
    "    \"\"\"\n",
    "    specs: List[ModelSpec] = []\n",
    "    if not os.path.exists(csv_path):\n",
    "        return specs\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception:\n",
    "        return specs\n",
    "\n",
    "    # Try to infer a size rank from a 'size' or 'parameters' column\n",
    "    def infer_size_rank(size_str: str) -> int:\n",
    "        if not isinstance(size_str, str):\n",
    "            return 5\n",
    "        s = size_str.lower()\n",
    "        # simple heuristics: lower rank is \"lighter\"\n",
    "        if any(k in s for k in [\"2b\", \"3b\", \"tiny\", \"mini\", \"small\"]): return 1\n",
    "        if any(k in s for k in [\"7b\", \"8b\", \"medium\"]): return 2\n",
    "        if any(k in s for k in [\"13b\", \"14b\", \"large\"]): return 3\n",
    "        if any(k in s for k in [\"30b\", \"34b\", \"xl\"]): return 4\n",
    "        return 5\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        call_name = str(row.get(\"call_name\") or row.get(\"name\") or \"\").strip()\n",
    "        size_str  = str(row.get(\"size\") or row.get(\"parameters\") or \"\").strip()\n",
    "        if not call_name:\n",
    "            continue\n",
    "        specs.append(\n",
    "            ModelSpec(\n",
    "                name=call_name,\n",
    "                provider=\"ollama\",\n",
    "                size_rank=infer_size_rank(size_str),\n",
    "                tags=[\"general\"],\n",
    "                est_cost_per_1k=0.0,        # local inference → treat as $0 for demo\n",
    "                est_latency=max(0.2, 0.05 * infer_size_rank(size_str)),\n",
    "                notes=f\"From CSV; size='{size_str}'\"\n",
    "            )\n",
    "        )\n",
    "    return specs\n",
    "\n",
    "def _default_catalog(allow_paid_models: bool = True) -> List[ModelSpec]:\n",
    "    \"\"\"\n",
    "    Default demo catalog if CSV is unavailable. Adjust to your environment.\n",
    "    \"\"\"\n",
    "    catalog = [\n",
    "        # Local (Ollama) — fast/light general chat\n",
    "        ModelSpec(name=\"llama3.2:1b\", provider=\"ollama\", size_rank=2,\n",
    "                  tags=[\"general\"], est_cost_per_1k=0.0, est_latency=0.3,\n",
    "                  notes=\"Local, lightweight general model\"),\n",
    "        # Local (Ollama) — code-aware (replace with your local favorite)\n",
    "        ModelSpec(name=\"codellama\", provider=\"ollama\", size_rank=2,\n",
    "                  tags=[\"code\"], est_cost_per_1k=0.0, est_latency=0.35,\n",
    "                  notes=\"Local, code-oriented\"),\n",
    "        # Local (Ollama) — medium general chat\n",
    "        ModelSpec(name=\"gpt-oss:20b\", provider=\"ollama\", size_rank=3,\n",
    "                  tags=[\"general\", \"code\", \"math\", \"complex\"], est_cost_per_1k=0.0, est_latency=0.5,\n",
    "                  notes=\"Local, medium size general model\")]\n",
    "    if allow_paid_models:\n",
    "      catalog += [\n",
    "          # Remote (OpenAI) — reasoning-heavy\n",
    "          ModelSpec(name=\"gpt-4o-mini\", provider=\"openai\", size_rank=1,\n",
    "                    tags=[\"general\", \"fast\"], est_cost_per_1k=0.6, est_latency=0.8,\n",
    "                    notes=\"Remote, budget/latency friendly\"),\n",
    "          # Remote (OpenAI) — fast/cheap\n",
    "          ModelSpec(name=\"gpt-4o\", provider=\"openai\", size_rank=4,\n",
    "                    tags=[\"reasoning\", \"math\", \"complex\"], est_cost_per_1k=5.0, est_latency=0.1,\n",
    "                    notes=\"Remote, strong reasoning\")\n",
    "          ]\n",
    "    return catalog\n",
    "\n",
    "def build_catalog(csv_path: str, allow_paid_models: bool = True) -> List[ModelSpec]:\n",
    "    specs = _read_ollama_csv(csv_path)\n",
    "    specs += _default_catalog(allow_paid_models)\n",
    "    return specs\n",
    "\n",
    "# -----------------------\n",
    "# Lightweight heuristics\n",
    "# -----------------------\n",
    "\n",
    "def is_complex_prompt(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Naive complexity signal: math markers, code fences, long length, or\n",
    "    words indicating step-by-step reasoning.\n",
    "    \"\"\"\n",
    "    if len(text) > 300:\n",
    "        return True\n",
    "    code_markers = [\"```\", \"def \", \"class \", \"SELECT \", \"FROM \", \"import \", \"public static void\"]\n",
    "    math_markers = [r\"\\bprove\\b\", r\"\\bderive\\b\", r\"\\bO\\(\", r\"\\bintegral\\b\", r\"[0-9]+\\s*[\\+\\-\\*\\/\\^]\\s*[0-9]+\"]\n",
    "    reasoning_terms = [r\"\\bchain of thought\\b\", r\"\\bstep-by-step\\b\", r\"\\bthink step by step\\b\"]\n",
    "\n",
    "    if any(m in text for m in code_markers):\n",
    "        return True\n",
    "    if any(re.search(p, text, flags=re.IGNORECASE) for p in math_markers + reasoning_terms):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def detect_specialization(text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Return a specialization tag when obvious (very naive).\n",
    "    \"\"\"\n",
    "    pairs = {\n",
    "        \"code\": [r\"\\bwrite (a|the) (python|js|sql|java|c\\+\\+|c#)\"],\n",
    "        \"sql\":  [r\"\\bSELECT\\b.*\\bFROM\\b\", r\"\\bCREATE TABLE\\b\"],\n",
    "        \"finance\": [r\"\\bDCF\\b\", r\"\\bWACC\\b\", r\"\\b10-K\\b\"],\n",
    "        \"biology\": [r\"\\bRNA\\b\", r\"\\bCRISPR\\b\"],\n",
    "        \"law\": [r\"\\bstatute\\b\", r\"\\btort\\b\"],\n",
    "        \"math\": [r\"\\bprove\\b\", r\"\\bintegral\\b\", r\"\\bderivative\\b\"],\n",
    "    }\n",
    "    for tag, patterns in pairs.items():\n",
    "        if any(re.search(p, text, flags=re.IGNORECASE | re.DOTALL) for p in patterns):\n",
    "            return tag\n",
    "    return None\n",
    "\n",
    "def snapshot_resources() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Quick resource snapshot (CPU %, available RAM GB). These values can inform routing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cpu = psutil.cpu_percent(interval=0.2)\n",
    "        mem = psutil.virtual_memory()\n",
    "        avail_gb = round(mem.available / (1024**3), 2)\n",
    "        return {\"cpu_percent\": cpu, \"available_gb\": avail_gb}\n",
    "    except Exception:\n",
    "        return {\"cpu_percent\": None, \"available_gb\": None}\n",
    "\n",
    "def score_candidate(\n",
    "    spec: ModelSpec,\n",
    "    *,\n",
    "    optimization: str,\n",
    "    complex_prompt: bool,\n",
    "    specialization: Optional[str],\n",
    "    resources: Dict[str, Any]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Lower score = better. Combine a few simple signals:\n",
    "      - If complex → prefer 'reasoning' models.\n",
    "      - If specialization detected → prefer related tags (e.g., 'code', 'math').\n",
    "      - Optimization: 'speed', 'cost', or 'depth' (reasoning depth).\n",
    "      - If CPU is high and provider == 'ollama' → small penalty (local load).\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "\n",
    "    # Based on optimization goal\n",
    "    if optimization == \"speed\":\n",
    "        score += spec.est_latency * 2.0 + spec.size_rank * 0.5\n",
    "    elif optimization == \"cost\":\n",
    "        score += spec.est_cost_per_1k * 1.5 + spec.size_rank * 0.3\n",
    "    elif optimization == \"depth\":\n",
    "        # prefer reasoning-heavy\n",
    "        score += (0 if (\"reasoning\" in spec.tags or \"complex\" in spec.tags) else 1.5)\n",
    "        score += spec.size_rank * 0.2\n",
    "    else:\n",
    "        # default blend\n",
    "        score += spec.est_latency + 0.2 * spec.est_cost_per_1k\n",
    "\n",
    "    # Complexity\n",
    "    if complex_prompt and not any(t in spec.tags for t in [\"reasoning\", \"complex\", \"math\", \"code\"]):\n",
    "        score += 1.0\n",
    "\n",
    "    # Specialization\n",
    "    if specialization:\n",
    "        if specialization in spec.tags or f\"specialized:{specialization}\" in spec.tags:\n",
    "            score -= 0.5  # small bonus\n",
    "        else:\n",
    "            # mild penalty if it doesn't look appropriate\n",
    "            score += 0.3\n",
    "\n",
    "    # Resource-aware: if CPU is busy and we're local, prefer remote\n",
    "    cpu = resources.get(\"cpu_percent\")\n",
    "    if cpu is not None and cpu > 70 and spec.provider == \"ollama\":\n",
    "        score += 0.8\n",
    "\n",
    "    # If RAM is very low and spec is large local model (size_rank>=3), penalize\n",
    "    avail_gb = resources.get(\"available_gb\")\n",
    "    if avail_gb is not None and avail_gb < 2.0 and spec.provider == \"ollama\" and spec.size_rank >= 3:\n",
    "        score += 0.7\n",
    "\n",
    "    return score\n",
    "\n",
    "# -----------------------\n",
    "# Dispatch\n",
    "# -----------------------\n",
    "\n",
    "def init_model(spec: ModelSpec):\n",
    "    \"\"\"\n",
    "    Initialize a ChatModel via LLMPop's init_llm().\n",
    "    For Ollama, auto_serve/auto_install/pull are enabled to make the demo smoother.\n",
    "    For OpenAI, use env OPENAI_API_KEY or pass provider_kwargs with api_key.\n",
    "    \"\"\"\n",
    "    if spec.provider == \"ollama\":\n",
    "        return init_llm(\n",
    "            model=spec.name,\n",
    "            provider=\"ollama\",\n",
    "            provider_kwargs={\n",
    "                \"auto_install\": True,\n",
    "                \"auto_serve\": True,\n",
    "                \"pull\": True,\n",
    "            },\n",
    "            temperature=0.0,\n",
    "        )  # :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "    elif spec.provider == \"openai\":\n",
    "        return init_llm(\n",
    "            model=spec.name,\n",
    "            provider=\"openai\",\n",
    "            temperature=0.0,\n",
    "        )  # :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {spec.provider}\")\n",
    "\n",
    "def call_model(chat_model, user_prompt: str, system_msg: str = \"You are a helpful assistant.\"):\n",
    "    \"\"\"\n",
    "    Call a LangChain ChatModel returned by init_llm(). Keep it simple with a short system + human turn.\n",
    "    \"\"\"\n",
    "    from langchain_core.messages import SystemMessage, HumanMessage\n",
    "    # Return raw content; for pure string you could add StrOutputParser (see guide). :contentReference[oaicite:4]{index=4}\n",
    "    result = chat_model.invoke([SystemMessage(content=system_msg), HumanMessage(content=user_prompt)])\n",
    "    return getattr(result, \"content\", str(result))\n",
    "\n",
    "# -----------------------\n",
    "# Main router\n",
    "# -----------------------\n",
    "\n",
    "def route_and_dispatch(\n",
    "    prompt: str,\n",
    "    *,\n",
    "    conversation_ctx: Optional[List[Dict[str, Any]]] = None,\n",
    "    optimization: str = \"speed\",   # 'speed' | 'cost' | 'depth'\n",
    "    runtime_info: Optional[Dict[str, Any]] = None,\n",
    "    csv_path: str = \"Ollama models by size and call name (a limited list).csv\",\n",
    "    allow_paid_models: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Core router: choose a model and execute the call. Returns a dict with\n",
    "    selection details and the model output.\n",
    "    \"\"\"\n",
    "    catalog = build_catalog(csv_path, allow_paid_models)\n",
    "\n",
    "    complex_prompt = is_complex_prompt(prompt)\n",
    "    specialization = detect_specialization(prompt)\n",
    "    resources = snapshot_resources()\n",
    "\n",
    "    # Incorporate any runtime quotas into a tiny penalty (dummy)\n",
    "    quotas = (runtime_info or {}).get(\"api_quotas\", {})\n",
    "    openai_quota_low = quotas.get(\"openai_remaining\", 1000) < 50\n",
    "    ollama_quota_low = quotas.get(\"ollama_remaining\", 1000) < 10\n",
    "\n",
    "    # Score each candidate\n",
    "    scored = []\n",
    "    for spec in catalog:\n",
    "        s = score_candidate(spec,\n",
    "                            optimization=optimization,\n",
    "                            complex_prompt=complex_prompt,\n",
    "                            specialization=specialization,\n",
    "                            resources=resources)\n",
    "        # Quotas as penalties (illustrative)\n",
    "        if spec.provider == \"openai\" and openai_quota_low:\n",
    "            s += 1.0\n",
    "        if spec.provider == \"ollama\" and ollama_quota_low:\n",
    "            s += 0.5\n",
    "        scored.append((s, spec))\n",
    "\n",
    "    scored.sort(key=lambda x: x[0])\n",
    "    chosen = scored[0][1]\n",
    "\n",
    "    # Try to init and call; fall back if needed\n",
    "    error = None\n",
    "    response = None\n",
    "    try:\n",
    "        chat_model = init_model(chosen)\n",
    "        response = call_model(chat_model, prompt)\n",
    "    except Exception as e:\n",
    "        error = f\"{type(e).__name__}: {e}\"\n",
    "        # Fallback: attempt the next candidate\n",
    "        if len(scored) > 1:\n",
    "            try:\n",
    "                alt = scored[1][1]\n",
    "                chat_model = init_model(alt)\n",
    "                response = call_model(chat_model, prompt)\n",
    "                chosen = alt\n",
    "            except Exception as e2:\n",
    "                response = f\"[Router demo fallback] Could not reach any LLM. Last error: {type(e2).__name__}: {e2}\"\n",
    "\n",
    "    return {\n",
    "        \"chosen_model\": chosen.name,\n",
    "        \"provider\": chosen.provider,\n",
    "        \"notes\": chosen.notes,\n",
    "        \"complex_prompt\": complex_prompt,\n",
    "        \"specialization\": specialization,\n",
    "        \"optimization\": optimization,\n",
    "        \"resources\": resources,\n",
    "        \"error\": error,\n",
    "        \"response\": response,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YiKJAo7hfm5",
    "outputId": "33f39e92-bbff-42b7-ff2c-1789b809fc19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Ollama server...\n",
      "→ Ollama PID: 634\n",
      "⏳ Waiting for Ollama to be ready…\n",
      "🚀 Starting Ollama server...\n",
      "→ Ollama PID: 637\n",
      "⏳ Waiting for Ollama to be ready…\n",
      "\n",
      "=== Router Decision ===\n",
      "Chosen: llama3.2:1b (provider=ollama)\n",
      "Notes:  Local, lightweight general model\n",
      "Complex prompt? False; specialization: None\n",
      "Optimization: speed\n",
      "Resources snapshot: {'cpu_percent': 1.3, 'available_gb': 21.17}\n",
      "Init error (first try): RuntimeError: Ollama server at http://127.0.0.1:11434 failed to start within 15s.\n",
      "\n",
      "=== Model Output (truncated) ===\n",
      "[Router demo fallback] Could not reach any LLM. Last error: RuntimeError: Ollama server at http://127.0.0.1:11434 failed to start within 15s.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Demo entrypoint\n",
    "# -----------------------\n",
    "\n",
    "def main(prompt: str,\n",
    "      conversation_ctx: Optional[List[Dict[str, Any]]] = None,\n",
    "      optimization: str = \"speed\",   # 'speed' | 'cost' | 'depth'\n",
    "      runtime_info: Optional[Dict[str, Any]] = None,\n",
    "      allow_paid_models: bool = True):\n",
    "    # # Optional: start a background resource logger (1 minute, every 5s) — for demo visibility.\n",
    "    # # This is purely illustrative; comment out if not desired.\n",
    "    # try:\n",
    "    #     _ = start_resource_monitoring(logfile=\"resource_usage.log\", duration=60, interval=5)  # :contentReference[oaicite:5]{index=5}\n",
    "    # except Exception:\n",
    "    #     pass\n",
    "\n",
    "    result = route_and_dispatch(\n",
    "        prompt=prompt,\n",
    "        conversation_ctx=conversation_ctx,\n",
    "        optimization=optimization,\n",
    "        runtime_info=runtime_info,\n",
    "        allow_paid_models=allow_paid_models\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Router Decision ===\")\n",
    "    print(f\"Chosen: {result['chosen_model']} (provider={result['provider']})\")\n",
    "    print(f\"Notes:  {result['notes']}\")\n",
    "    print(f\"Complex prompt? {result['complex_prompt']}; specialization: {result['specialization']}\")\n",
    "    print(f\"Optimization: {result['optimization']}\")\n",
    "    print(f\"Resources snapshot: {result['resources']}\")\n",
    "    if result[\"error\"]:\n",
    "        print(f\"Init error (first try): {result['error']}\")\n",
    "\n",
    "    print(\"\\n=== Model Output (truncated) ===\")\n",
    "    print(str(result[\"response\"])[:1200])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample inputs (edit these to experiment)\n",
    "    prompt = (\n",
    "        \"Write a short Python function that returns the nth Fibonacci number, \"\n",
    "        \"then analyze the time complexity.\"\n",
    "    )\n",
    "    conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
    "    optimization = \"speed\"  # try 'speed', 'cost', or 'depth'\n",
    "\n",
    "    # Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
    "    runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
    "    allow_paid_models = True\n",
    "    main(prompt, conversation_ctx, optimization, runtime_info, allow_paid_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBq0T9sOYcgZ"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2yn_tVSVKj7"
   },
   "source": [
    "## Evaluating Various Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RQt3rbGVQJK"
   },
   "source": [
    "### Optimize depth: coding task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQGgeSv0hAEF",
    "outputId": "990588f7-0d2c-40af-e1fa-c908a39c21ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Ollama server...\n",
      "→ Ollama PID: 641\n",
      "⏳ Waiting for Ollama to be ready…\n",
      "🚀 Setting up remote OpenAI chat model…\n",
      "All done setting up OpenAI (ChatOpenAI).\n",
      "\n",
      "\n",
      "=== Router Decision ===\n",
      "Chosen: gpt-4o (provider=openai)\n",
      "Notes:  Remote, strong reasoning\n",
      "Complex prompt? False; specialization: None\n",
      "Optimization: depth\n",
      "Resources snapshot: {'cpu_percent': 0.0, 'available_gb': 21.16}\n",
      "Init error (first try): RuntimeError: Ollama server at http://127.0.0.1:11434 failed to start within 15s.\n",
      "\n",
      "=== Model Output (truncated) ===\n",
      "Certainly! Below is a Python function that returns the nth Fibonacci number using an iterative approach:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    \n",
      "    a, b = 0, 1\n",
      "    for _ in range(2, n + 1):\n",
      "        a, b = b, a + b\n",
      "    return b\n",
      "```\n",
      "\n",
      "### Time Complexity Analysis\n",
      "\n",
      "The time complexity of this function is \\(O(n)\\). Here's why:\n",
      "\n",
      "- The function uses a loop that iterates from 2 to \\(n\\) (inclusive), which means it runs \\(n - 1\\) times.\n",
      "- Each iteration of the loop involves a constant amount of work: updating the values of `a` and `b`.\n",
      "- Therefore, the time complexity is linear with respect to \\(n\\), or \\(O(n)\\).\n",
      "\n",
      "This approach is efficient for calculating Fibonacci numbers because it avoids the exponential time complexity of the naive recursive approach, which is \\(O(2^n)\\). The iterative method also uses constant space, \\(O(1)\\), since it only requires a fixed amount of additional memory regardless of the input size.\n"
     ]
    }
   ],
   "source": [
    "# Sample inputs (edit these to experiment)\n",
    "prompt = (\n",
    "    \"Write a short Python function that returns the nth Fibonacci number, \"\n",
    "    \"then analyze the time complexity.\"\n",
    ")\n",
    "conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
    "optimization = \"depth\"  # try 'speed', 'cost', or 'depth'\n",
    "\n",
    "# Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
    "runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
    "allow_paid_models = True\n",
    "main(prompt, conversation_ctx, optimization, runtime_info, allow_paid_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSLodW9kYLa0"
   },
   "source": [
    "### Optimize depth: coding task  \n",
    "Excluding paid remote models, sticking with local models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2BoSrrj4YLa0",
    "outputId": "f7cf5da3-7974-418d-f8f4-fa92437c7636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Ollama server...\n",
      "→ Ollama PID: 654\n",
      "⏳ Waiting for Ollama to be ready…\n",
      "🚀 Starting Ollama server...\n",
      "→ Ollama PID: 656\n",
      "⏳ Waiting for Ollama to be ready…\n",
      "\n",
      "=== Router Decision ===\n",
      "Chosen: gpt-oss:20b (provider=ollama)\n",
      "Notes:  Local, medium size general model\n",
      "Complex prompt? False; specialization: None\n",
      "Optimization: depth\n",
      "Resources snapshot: {'cpu_percent': 1.2, 'available_gb': 20.89}\n",
      "Init error (first try): RuntimeError: Ollama server at http://127.0.0.1:11434 failed to start within 15s.\n",
      "\n",
      "=== Model Output (truncated) ===\n",
      "[Router demo fallback] Could not reach any LLM. Last error: RuntimeError: Ollama server at http://127.0.0.1:11434 failed to start within 15s.\n"
     ]
    }
   ],
   "source": [
    "# Sample inputs (edit these to experiment)\n",
    "prompt = (\n",
    "    \"Write a short Python function that returns the nth Fibonacci number, \"\n",
    "    \"then analyze the time complexity.\"\n",
    ")\n",
    "conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
    "optimization = \"depth\"  # try 'speed', 'cost', or 'depth'\n",
    "\n",
    "# Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
    "runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
    "allow_paid_models = False\n",
    "main(prompt, conversation_ctx, optimization, runtime_info, allow_paid_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JD2QkkflXpR7"
   },
   "source": [
    "### Optimize cost: Math puzzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kVjmIH5-XpR7",
    "outputId": "6ff3e67b-25e2-474c-99c7-319e73cd6e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Ollama server...\n",
      "→ Ollama PID: 658\n",
      "⏳ Waiting for Ollama to be ready…\n",
      "🚀 Starting Ollama server...\n",
      "→ Ollama PID: 664\n",
      "⏳ Waiting for Ollama to be ready…\n",
      "\n",
      "=== Router Decision ===\n",
      "Chosen: gpt-oss:20b (provider=ollama)\n",
      "Notes:  Local, medium size general model\n",
      "Complex prompt? True; specialization: math\n",
      "Optimization: cost\n",
      "Resources snapshot: {'cpu_percent': 2.4, 'available_gb': 20.88}\n",
      "Init error (first try): RuntimeError: Ollama server at http://127.0.0.1:11434 failed to start within 15s.\n",
      "\n",
      "=== Model Output (truncated) ===\n",
      "[Router demo fallback] Could not reach any LLM. Last error: RuntimeError: Ollama server at http://127.0.0.1:11434 failed to start within 15s.\n"
     ]
    }
   ],
   "source": [
    "# Sample inputs (edit these to experiment)\n",
    "prompt = (\n",
    "    \"Prove that the squary root of 2 is irrational.\"\n",
    "    \"Then simplify it in one sentence for a 6 year old.\"\n",
    ")\n",
    "conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
    "optimization = \"cost\"  # try 'speed', 'cost', or 'depth'\n",
    "\n",
    "# Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
    "runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
    "allow_paid_models = True\n",
    "main(prompt, conversation_ctx, optimization, runtime_info, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b20lrxUebTPz"
   },
   "source": [
    "### Optimize speed: Math puzzle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gQ48RfhDbTP0",
    "outputId": "cf0ac745-3e43-41f3-ea28-7bc2d9349e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Setting up remote OpenAI chat model…\n",
      "All done setting up OpenAI (ChatOpenAI).\n",
      "\n",
      "\n",
      "=== Router Decision ===\n",
      "Chosen: gpt-4o (provider=openai)\n",
      "Notes:  Remote, strong reasoning\n",
      "Complex prompt? True; specialization: math\n",
      "Optimization: speed\n",
      "Resources snapshot: {'cpu_percent': 0.0, 'available_gb': 20.88}\n",
      "\n",
      "=== Model Output (truncated) ===\n",
      "To prove that the square root of 2 is irrational, we can use a proof by contradiction. Assume that \\(\\sqrt{2}\\) is rational, meaning it can be expressed as a fraction \\(\\frac{a}{b}\\) where \\(a\\) and \\(b\\) are integers with no common factors other than 1, and \\(b \\neq 0\\). Then we have:\n",
      "\n",
      "\\[\n",
      "\\sqrt{2} = \\frac{a}{b}\n",
      "\\]\n",
      "\n",
      "Squaring both sides gives:\n",
      "\n",
      "\\[\n",
      "2 = \\frac{a^2}{b^2}\n",
      "\\]\n",
      "\n",
      "Multiplying both sides by \\(b^2\\) gives:\n",
      "\n",
      "\\[\n",
      "2b^2 = a^2\n",
      "\\]\n",
      "\n",
      "This implies that \\(a^2\\) is even, which means \\(a\\) must be even (since the square of an odd number is odd). So, let \\(a = 2k\\) for some integer \\(k\\). Substituting back, we get:\n",
      "\n",
      "\\[\n",
      "2b^2 = (2k)^2 = 4k^2\n",
      "\\]\n",
      "\n",
      "Dividing both sides by 2 gives:\n",
      "\n",
      "\\[\n",
      "b^2 = 2k^2\n",
      "\\]\n",
      "\n",
      "This implies that \\(b^2\\) is even, so \\(b\\) must also be even. However, if both \\(a\\) and \\(b\\) are even, they have a common factor of 2, which contradicts our assumption that \\(\\frac{a}{b}\\) is in its simplest form. Therefore, \\(\\sqrt{2}\\) cannot be expressed as a fraction of two integers, and is thus irrational.\n",
      "\n",
      "For a 6-year-old: \"The square root of 2 is a number that can't be made by dividing one whole number by another, no matter how hard you try!\"\n"
     ]
    }
   ],
   "source": [
    "# Sample inputs (edit these to experiment)\n",
    "prompt = (\n",
    "    \"Prove that the squary root of 2 is irrational.\"\n",
    "    \"Then simplify it in one sentence for a 6 year old.\"\n",
    ")\n",
    "conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
    "optimization = \"speed\"  # try 'speed', 'cost', or 'depth'\n",
    "\n",
    "# Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
    "runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
    "allow_paid_models = True\n",
    "main(prompt, conversation_ctx, optimization, runtime_info, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FWsmYA5bTlE"
   },
   "source": [
    "### Optimize speed: Math puzzle\n",
    "Excluding paid remote models, sticking with local models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NA0-gLSibTlE",
    "outputId": "15400a11-1e76-43f8-dfd8-1ca426f75ab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Ollama server...\n",
      "→ Ollama PID: 666\n",
      "⏳ Waiting for Ollama to be ready…\n",
      "🚀 Starting Ollama server...\n",
      "→ Ollama PID: 668\n",
      "⏳ Waiting for Ollama to be ready…\n",
      "\n",
      "=== Router Decision ===\n",
      "Chosen: gpt-oss:20b (provider=ollama)\n",
      "Notes:  Local, medium size general model\n",
      "Complex prompt? True; specialization: math\n",
      "Optimization: cost\n",
      "Resources snapshot: {'cpu_percent': 2.5, 'available_gb': 20.88}\n",
      "Init error (first try): RuntimeError: Ollama server at http://127.0.0.1:11434 failed to start within 15s.\n",
      "\n",
      "=== Model Output (truncated) ===\n",
      "[Router demo fallback] Could not reach any LLM. Last error: RuntimeError: Ollama server at http://127.0.0.1:11434 failed to start within 15s.\n"
     ]
    }
   ],
   "source": [
    "# Sample inputs (edit these to experiment)\n",
    "prompt = (\n",
    "    \"Prove that the squary root of 2 is irrational.\"\n",
    "    \"Then simplify it in one sentence for a 6 year old.\"\n",
    ")\n",
    "conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
    "optimization = \"cost\"  # try 'speed', 'cost', or 'depth'\n",
    "\n",
    "# Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
    "runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
    "allow_paid_models = False\n",
    "main(prompt, conversation_ctx, optimization, runtime_info, )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "948vnVL3fCJU",
    "0Akit5rDfOtR"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (sandbox_workshop)",
   "language": "python",
   "name": "sandbox_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
