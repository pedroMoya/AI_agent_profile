{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyeCwDW69Zf3"
   },
   "source": [
    "# Demo 3 - Monitoring & Tracing Example, and Model Differences\n",
    "By: [Lior Gazit](https://github.com/LiorGazit).  \n",
    "Repo: [Agents-Over-The-Weekend](https://github.com/PacktPublishing/Agents-Over-The-Weekend/tree/main/Lior_Gazit/workshop_september_2025/)  \n",
    "Running LLMs locally for free: This code leverages [`LLMPop`](https://pypi.org/project/llmpop/) that is dedicated to spinning up local or remote LLMs in a unified and modular syntax.  \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PacktPublishing/Agents-Over-The-Weekend/blob/main/Lior_Gazit/workshop_september_2025/codes_for_Lior_Bootcamp_talk_sept2025_demo3.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a> (pick a GPU Colab session for fastest computing)  \n",
    "\n",
    "```\n",
    "Disclaimer: The content and ideas presented in this notebook are solely those of the author, Lior Gazit, and do not represent the views or intellectual property of the author's employer.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn9qIRLS9Zf7"
   },
   "source": [
    "Installing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMpPyY829Zf9",
    "outputId": "47e0fc2b-abd0-4bc1-eba3-69b29c14b137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install llmpop\n",
    "%pip -q install langchain langchain_core tiktoken langsmith langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uU75SUy9ZgA"
   },
   "source": [
    "**Imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVDmCWnO9ZgB",
    "outputId": "faaaec7b-4cc1-4893-986a-595eb3ed5b71"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWO5Ngf2tDKb"
   },
   "source": [
    "Code example using LangSmiths's trace support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwMTGwwuvgCY",
    "outputId": "82bff6aa-067f-4029-b941-686bd6e4eff2"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste your LangChain API key:  ········\n",
      "Paste your OpenAI API key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting tracing for project <multi-agent-demo05>, funtion <multi_agent_interaction>\n",
      "\n",
      "Task for coder to perform:\n",
      "Write a Python function `reverse_string(s)` that returns the reverse of the string.\n",
      "\n",
      "=== Coder Output (Time: 1.99s, Tokens: 41) ===\n",
      "\n",
      "Certainly! Here's a concise Python function to reverse a string:\n",
      "\n",
      "```python\n",
      "def reverse_string(s):\n",
      "    return s[::-1]\n",
      "```\n",
      "\n",
      "This function uses Python's slicing feature to reverse the string.\n",
      "\n",
      "=== Reviewer Feedback (Time: 8.83s, Tokens: 379) ===\n",
      "\n",
      "The provided Python function `reverse_string` is a concise and efficient way to reverse a string using Python's slicing feature. However, there are a few considerations and potential improvements that could be made, depending on the context in which this function is used:\n",
      "\n",
      "1. **Input Validation**: \n",
      "   - If the function is expected to handle only strings, you might want to add a check to ensure that the input is indeed a string. This can help prevent unexpected behavior or errors if the function is called with a non-string argument.\n",
      "\n",
      "   ```python\n",
      "   def reverse_string(s):\n",
      "       if not isinstance(s, str):\n",
      "           raise TypeError(\"Input must be a string\")\n",
      "       return s[::-1]\n",
      "   ```\n",
      "\n",
      "2. **Documentation**:\n",
      "   - Adding a docstring to the function can improve code readability and provide guidance on how to use the function.\n",
      "\n",
      "   ```python\n",
      "   def reverse_string(s):\n",
      "       \"\"\"\n",
      "       Reverses the given string.\n",
      "\n",
      "       Parameters:\n",
      "       s (str): The string to be reversed.\n",
      "\n",
      "       Returns:\n",
      "       str: The reversed string.\n",
      "\n",
      "       Raises:\n",
      "       TypeError: If the input is not a string.\n",
      "       \"\"\"\n",
      "       if not isinstance(s, str):\n",
      "           raise TypeError(\"Input must be a string\")\n",
      "       return s[::-1]\n",
      "   ```\n",
      "\n",
      "3. **Edge Cases**:\n",
      "   - Consider how the function should behave with edge cases, such as an empty string or very large strings. The current implementation handles these cases correctly, but it's good to be aware of them.\n",
      "\n",
      "4. **Performance**:\n",
      "   - The slicing method is already quite efficient for reversing strings in Python. However, if performance becomes a concern with very large strings, you might consider alternative methods or optimizations, though for most practical purposes, slicing is sufficient.\n",
      "\n",
      "Overall, the function is well-written for its purpose, and the suggestions above are mainly for robustness and clarity.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import openai\n",
    "\n",
    "# Set up environment variables (make sure your keys are set correctly)\n",
    "if \"langchain_api_key\" not in globals():\n",
    "  langchain_api_key = getpass(\"Paste your LangChain API key: \")\n",
    "if not openai.api_key:\n",
    "  openai.api_key = getpass(\"Paste your OpenAI API key: \")\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"]=\"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
    "# IMPORTANT: If you change the designated project, you must restart the notebook kernel.\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"multi-agent-demo05\"\n",
    "\n",
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable\n",
    "from langsmith.run_helpers import trace\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import tiktoken\n",
    "\n",
    "# Helper to count tokens\n",
    "def count_tokens(text, encoding_name=\"cl100k_base\"):\n",
    "    enc = tiktoken.get_encoding(encoding_name)\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "# Setup LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Prompts for agents\n",
    "coder_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a coding assistant. Write concise Python code for this task:\\n{task}\"\n",
    ")\n",
    "\n",
    "reviewer_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a meticulous code reviewer. Identify bugs or improvements in the following code:\\n{code}\"\n",
    ")\n",
    "\n",
    "# Chains\n",
    "coder_chain = coder_prompt | llm\n",
    "reviewer_chain = reviewer_prompt | llm\n",
    "\n",
    "# Traceable agent function\n",
    "@traceable(name=\"multi_agent_interaction\")\n",
    "def multi_agent_interaction(task):\n",
    "    # Coder\n",
    "    start_coder = time.time()\n",
    "    coder_response = coder_chain.invoke({\"task\": task})\n",
    "    coder_duration = time.time() - start_coder\n",
    "    coder_code = coder_response.content\n",
    "\n",
    "    print(f\"\\n=== Coder Output (Time: {coder_duration:.2f}s, Tokens: {count_tokens(coder_code)}) ===\\n\")\n",
    "    print(coder_code)\n",
    "\n",
    "    # Reviewer\n",
    "    start_reviewer = time.time()\n",
    "    reviewer_response = reviewer_chain.invoke({\"code\": coder_code})\n",
    "    reviewer_duration = time.time() - start_reviewer\n",
    "    reviewer_feedback = reviewer_response.content\n",
    "\n",
    "    print(f\"\\n=== Reviewer Feedback (Time: {reviewer_duration:.2f}s, Tokens: {count_tokens(reviewer_feedback)}) ===\\n\")\n",
    "    print(reviewer_feedback)\n",
    "\n",
    "# Execute with trace context\n",
    "with trace(\"multi_agent_demo_run\"):\n",
    "    print(\"\\nStarting tracing for project <\" + os.environ[\"LANGCHAIN_PROJECT\"] + \">, funtion <multi_agent_interaction>\")\n",
    "    task_description = \"Write a Python function `reverse_string(s)` that returns the reverse of the string.\"\n",
    "    print(f\"\\nTask for coder to perform:\\n{task_description}\")\n",
    "    multi_agent_interaction(task_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F11TLCs39ZgK"
   },
   "source": [
    "Code example for where building the logging process ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7EXGKqf9ZgK",
    "outputId": "9662ff77-5269-48cd-9f54-740a514a918d"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Ollama server at http://127.0.0.1:11434 failed to start within 15s.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(record) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 4. Load your model (local Ollama example)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43minit_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCodeLlama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mollama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 5. Step 1: Coder agent (generate code)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m step1_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a Python function `reverse_string(s)` that returns the reverse of s.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/d/sandbox_workshop/.venv/lib/python3.10/site-packages/llmpop/init_llm.py:153\u001b[0m, in \u001b[0;36minit_llm\u001b[0;34m(model, provider, provider_kwargs, verbose, **chat_init_kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⏳ Waiting for Ollama to be ready…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m \u001b[43mwait_for_ollama_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReady!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/d/sandbox_workshop/.venv/lib/python3.10/site-packages/llmpop/init_llm.py:24\u001b[0m, in \u001b[0;36mwait_for_ollama_ready\u001b[0;34m(host, port, timeout)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError:\n\u001b[1;32m     23\u001b[0m         sleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama server at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed to start within \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Ollama server at http://127.0.0.1:11434 failed to start within 15s."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "import json\n",
    "import tiktoken\n",
    "from llmpop import init_llm\n",
    "\n",
    "\n",
    "# 1. Basic logging setup\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "# 2. Helper: count tokens using tiktoken\n",
    "def count_tokens(text, encoding_name=\"cl100k_base\"):\n",
    "    enc = tiktoken.get_encoding(encoding_name)\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "# 3. Helper: log each call\n",
    "def log_call(step_name, prompt, response, start, end, log_file=\"llm_trace.log\"):\n",
    "    record = {\n",
    "        \"step\": step_name,\n",
    "        \"prompt_tokens\": count_tokens(prompt),\n",
    "        \"response_tokens\": count_tokens(response),\n",
    "        \"duration_s\": round(end - start, 3),\n",
    "        \"timestamp\": start\n",
    "    }\n",
    "    # Console output\n",
    "    logging.info(f\"[{step_name}] {record['duration_s']}s | \"\n",
    "                 f\"prompt_tokens={record['prompt_tokens']} | \"\n",
    "                 f\"response_tokens={record['response_tokens']}\")\n",
    "    # Append to JSON‑lines file\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "# 4. Load your model (local Ollama example)\n",
    "model = init_llm(model=\"CodeLlama\", provider=\"ollama\", verbose=False)\n",
    "\n",
    "# 5. Step 1: Coder agent (generate code)\n",
    "step1_prompt = \"Write a Python function `reverse_string(s)` that returns the reverse of s.\"\n",
    "start = time.time()\n",
    "step1_response = model.invoke(step1_prompt)\n",
    "end = time.time()\n",
    "log_call(\"Coder\", step1_prompt, step1_response.content, start, end)\n",
    "\n",
    "# 6. Step 2: Reviewer agent (review code)\n",
    "step2_prompt = f\"Review this code for correctness and edge cases:\\n\\n{step1_response}\"\n",
    "start = time.time()\n",
    "step2_response = model.invoke(step2_prompt)\n",
    "end = time.time()\n",
    "log_call(\"Reviewer\", step2_prompt, step2_response.content, start, end)\n",
    "\n",
    "# 7. Print outputs\n",
    "print(\"=== Coder’s Code ===\\n\", step1_response.content)\n",
    "print(\"\\n=== Reviewer’s Feedback ===\\n\", step2_response.content)\n",
    "\n",
    "# 8. Inspect the log file if desired:\n",
    "print(\"\\n---\\nPrinting the log:\")\n",
    "!head -n 10 llm_trace.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsmlGaa3AYxt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (sandbox_workshop)",
   "language": "python",
   "name": "sandbox_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
