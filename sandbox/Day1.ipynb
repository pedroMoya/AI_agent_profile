{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-eAzqwVZeMT"
   },
   "source": [
    "# Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OSmN3fiSZfs4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLANGSMITH_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m userdata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLANGSMITH_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLANGSMITH_TRACING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"PUT YOURS HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFY9j5sphPKq"
   },
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVuy31nOhRZZ"
   },
   "source": [
    "Let's invoke the Gemini with a simple text input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbREhv7fgLAG"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rb263mc9g1gR"
   },
   "outputs": [],
   "source": [
    "google_api_key = userdata.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjYpwP5NgYOa"
   },
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXWF2rhmgm10",
    "outputId": "485679cf-3e61-457d-a7b3-7c91ad2ac0ea"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "isinstance(llm, Runnable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWF1ARONgtNa",
    "outputId": "22acb269-23e0-4392-9632-3eb956dfa0a5"
   },
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is the capital of the USA?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LXQY5QUEg0Fy",
    "outputId": "6fac72ac-3158-43a2-a340-f20d2f7271dd"
   },
   "outputs": [],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sk7FS0iahBvf",
    "outputId": "707334d4-7748-405c-c6b8-fa416822ba60"
   },
   "outputs": [],
   "source": [
    "print(result.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEnlwE3DhXW2"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "user_input = HumanMessage(content=\"What is the capital of the USA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcGqxRmGhhS_"
   },
   "outputs": [],
   "source": [
    "step1 = llm.invoke([user_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "B4hfthOzIzLy",
    "outputId": "6b4f9d0a-fd3a-4495-c030-1bde78bebb77"
   },
   "outputs": [],
   "source": [
    "type(step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q47z6YusI1nI",
    "outputId": "ac092a1a-004b-4d8f-c362-9a12a7ddbaec"
   },
   "outputs": [],
   "source": [
    "print(step1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1QwT04PI3AF",
    "outputId": "03c36063-933a-4d5f-b004-d15cda83469a"
   },
   "outputs": [],
   "source": [
    "print(step1.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_iXIAEMlI7OU",
    "outputId": "9bc97b8c-cf82-47e6-c550-5514cfa15c38"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = (\n",
    "    \"Be concise and answer user's question carefully.\\n\\n\"\n",
    "    \"QUESTION:\\n{question}\\n\"\n",
    ")\n",
    "\n",
    "question = \"What is the capital of the USA?\"\n",
    "lc_prompt_template = PromptTemplate.from_template(prompt_template)\n",
    "lc_prompt_template.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABrQCaQ5JNnh"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = lc_prompt_template | llm | StrOutputParser()\n",
    "result = chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEtkG5tKiNmF",
    "outputId": "42c1dba1-f447-432a-c977-3a8889e2c4ae"
   },
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tp51JXL-KORI"
   },
   "source": [
    "A special placeholder for messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0DkjU2hKNzO",
    "outputId": "cb9a7b69-957c-4915-de8c-3f52b7815559"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "\n",
    "msg_template = HumanMessagePromptTemplate.from_template(prompt_template)\n",
    "msg_example = msg_template.format(question=question)\n",
    "\n",
    "print(msg_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "6xzdD0GKKYXt",
    "outputId": "749bea51-acc0-4998-8861-357951d22eb2"
   },
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([SystemMessage(content=\"You are a helpful assistant.\"), msg_template])\n",
    "chain = chat_prompt_template | llm | StrOutputParser()\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2ipEzczKfrp"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are a helpful assistant.\"),\n",
    "     (\"placeholder\", \"{history}\"),\n",
    "     # same as MessagesPlaceholder(\"history\"),\n",
    "     (\"human\", prompt_template)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5eUU2TnKh1p",
    "outputId": "5abfe821-b4d0-4a7f-ac4a-945964d21b4d"
   },
   "outputs": [],
   "source": [
    "chat_prompt_template.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EeBas7GtKlj3",
    "outputId": "3c5eab8a-0d5a-4e15-d42f-ac690a374331"
   },
   "outputs": [],
   "source": [
    "len(chat_prompt_template.invoke(question).messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJfrW_jgKqMG",
    "outputId": "99c626e0-6c31-4cf7-d5ae-db9d35bd26ad"
   },
   "outputs": [],
   "source": [
    "len(chat_prompt_template.invoke({\"question\": question, \"history\": [(\"user\", \"hi\"), (\"ai\", \"how can I help?\")]}).messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANH-OhFBK1hx"
   },
   "outputs": [],
   "source": [
    "def increment_by_one(x: int) -> int:\n",
    " return x + 1\n",
    "\n",
    "\n",
    "def fake_llm(x: int) -> str:\n",
    " return f\"Result = {x}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9s-IFCgN6ho",
    "outputId": "76b69c70-c4da-4671-8ecf-8a5cc79cbca2"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = (\n",
    "   increment_by_one | RunnableLambda(fake_llm)\n",
    ")\n",
    "\n",
    "\n",
    "result = chain.invoke(1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqbUgSBKOKwM",
    "outputId": "bc9b268e-8c6e-4aa5-bec7-317e707227aa"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "\n",
    "a = increment_by_one | RunnableLambda(fake_llm)\n",
    "b = RunnableSequence(RunnableLambda(increment_by_one), RunnableLambda(fake_llm))\n",
    "\n",
    "print(a == b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eL2MLZOOPV8"
   },
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "\n",
    "cb = UsageMetadataCallbackHandler()\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\", google_api_key=google_api_key, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LuekYPRSRusf",
    "outputId": "8a12b922-3a70-487d-e036-bb6b3327e02a"
   },
   "outputs": [],
   "source": [
    "chain = lc_prompt_template | llm | StrOutputParser()\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4kv0sFJERvft",
    "outputId": "fe2c6dce-ea0b-413f-c819-662250a5ad34"
   },
   "outputs": [],
   "source": [
    "print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3ydNlMHr5KH"
   },
   "source": [
    "# Intro to LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "wXX4tGjfr6xA",
    "outputId": "8e616fce-be62-475c-d13e-4514b95a7267"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class CustomState(TypedDict):\n",
    "    a: int\n",
    "    b: int\n",
    "    result: int\n",
    "\n",
    "\n",
    "def _node_a(state):\n",
    "    return {\"a\": 1}\n",
    "\n",
    "def _node_b(state):\n",
    "    return {\"b\": 2}\n",
    "\n",
    "def _node_sum(state):\n",
    "    a = state[\"a\"]\n",
    "    b = state[\"b\"]\n",
    "    return {\"result\": a+b}\n",
    "\n",
    "builder = StateGraph(CustomState)\n",
    "builder.add_node(\"node_a\", _node_a)\n",
    "builder.add_node(\"node_b\", _node_b)\n",
    "builder.add_node(\"node_sum\", _node_sum)\n",
    "\n",
    "builder.add_edge(START, \"node_a\")\n",
    "builder.add_edge(START, \"node_b\")\n",
    "builder.add_edge(\"node_a\", \"node_sum\")\n",
    "builder.add_edge(\"node_b\", \"node_sum\")\n",
    "builder.add_edge(\"node_sum\", END)\n",
    "\n",
    "workflow = builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13IekX73zFaz",
    "outputId": "88b01d09-a5ac-4c10-e81e-e3b94c07e5dd"
   },
   "outputs": [],
   "source": [
    "for event in workflow.stream({}, stream_mode=\"values\"):\n",
    "  print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "D_PpvaYNFEsT",
    "outputId": "022899c8-7e3c-4746-be9c-601607e24e5f"
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "class CustomState(TypedDict):\n",
    "    operation: str\n",
    "    a: int\n",
    "    b: int\n",
    "    result: int\n",
    "\n",
    "def _node_multiply(state):\n",
    "    a = state[\"a\"]\n",
    "    b = state[\"b\"]\n",
    "    return {\"result\": a*b}\n",
    "\n",
    "def _edge(state) -> Literal[\"node_sum\", \"node_multiply\"]:\n",
    "    if state[\"operation\"] == \"sum\":\n",
    "      return \"node_sum\"\n",
    "    return \"node_multiply\"\n",
    "\n",
    "builder = StateGraph(CustomState)\n",
    "builder.add_node(\"node_a\", _node_a)\n",
    "builder.add_node(\"node_b\", _node_b)\n",
    "builder.add_node(\"node_sum\", _node_sum)\n",
    "builder.add_node(\"node_multiply\", _node_multiply)\n",
    "\n",
    "builder.add_edge(START, \"node_a\")\n",
    "builder.add_edge(START, \"node_b\")\n",
    "builder.add_conditional_edges(\"node_a\", _edge)\n",
    "builder.add_conditional_edges(\"node_b\", _edge)\n",
    "builder.add_edge(\"node_sum\", END)\n",
    "builder.add_edge(\"node_multiply\", END)\n",
    "\n",
    "workflow = builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcKuzoqzFF8T",
    "outputId": "c358002d-0e63-4f46-f211-59b04d1908ae"
   },
   "outputs": [],
   "source": [
    "initial_state = {\"operation\": \"add\"}\n",
    "for event in workflow.stream(initial_state, stream_mode=\"values\"):\n",
    "  print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TvPfUigiFaZo",
    "outputId": "1cf32266-fea7-400d-a7d3-9920b193a4cc"
   },
   "outputs": [],
   "source": [
    "for event in workflow.stream({'operation': 'multiply'}, stream_mode=\"values\"):\n",
    "  print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L55piTzpzbyO",
    "outputId": "41c2abea-8a62-4817-cdcb-fc7a707be64b"
   },
   "outputs": [],
   "source": [
    "result = await workflow.ainvoke(initial_state)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Z47F4MMEclR"
   },
   "source": [
    "Now, let's take at reducers. We saw the default reducer - it replaces the value in the state. Another option is to use a built-in reducer, for example `add` with a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1eJTXNJlu75",
    "outputId": "3558ae44-185f-471a-8117-5dec1f61f953"
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "add([1, 2], [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "z6v3YakjEMav",
    "outputId": "a5ceb75a-8b3e-49df-e6f5-fa9fe06197aa"
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from typing import Annotated\n",
    "\n",
    "class CustomState(TypedDict):\n",
    "    values: Annotated[list[int], add]\n",
    "    result: int\n",
    "\n",
    "\n",
    "def _node_a(state):\n",
    "    return {\"values\": [1]}\n",
    "\n",
    "def _node_b(state):\n",
    "    return {\"values\": [2]}\n",
    "\n",
    "def _node_sum(state):\n",
    "    return {\"result\": sum(state[\"values\"])}\n",
    "\n",
    "builder = StateGraph(CustomState)\n",
    "builder.add_node(\"node_a\", _node_a)\n",
    "builder.add_node(\"node_b\", _node_b)\n",
    "builder.add_node(\"node_sum\", _node_sum)\n",
    "\n",
    "builder.add_edge(START, \"node_a\")\n",
    "builder.add_edge(\"node_a\", \"node_b\")\n",
    "builder.add_edge(\"node_b\", \"node_sum\")\n",
    "builder.add_edge(\"node_sum\", END)\n",
    "\n",
    "workflow = builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZELGTJDfGAjv",
    "outputId": "90bd1c38-8db2-455b-c3c3-cf23a37e0123"
   },
   "outputs": [],
   "source": [
    "for event in workflow.stream({}, stream_mode=\"values\"):\n",
    "  print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L11YZ0T0Gmrf"
   },
   "source": [
    "Now, let's take a look at custom reducers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhY8hybpGOBs"
   },
   "outputs": [],
   "source": [
    "def my_reducer(left: int, right: int) -> int:\n",
    "  if right:\n",
    "    return left + right\n",
    "  return left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "DarbOmATGZGA",
    "outputId": "4430676f-0e1d-4331-abb6-65e55e965105"
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from typing import Annotated\n",
    "\n",
    "class CustomState(TypedDict):\n",
    "    value: Annotated[int, my_reducer]\n",
    "\n",
    "\n",
    "def _node_a(state):\n",
    "    return {\"value\": 1}\n",
    "\n",
    "def _node_b(state):\n",
    "    return {\"value\": 2}\n",
    "\n",
    "builder = StateGraph(CustomState)\n",
    "builder.add_node(\"node_a\", _node_a)\n",
    "builder.add_node(\"node_b\", _node_b)\n",
    "\n",
    "builder.add_edge(START, \"node_a\")\n",
    "builder.add_edge(\"node_a\", \"node_b\")\n",
    "builder.add_edge(\"node_b\", END)\n",
    "\n",
    "workflow = builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEMi_n_SGgNn",
    "outputId": "b0a4c6dd-455b-401e-a465-19a67deb1887"
   },
   "outputs": [],
   "source": [
    "for event in workflow.stream({}, stream_mode=\"values\"):\n",
    "  print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9g2EZH_Xhk8L"
   },
   "outputs": [],
   "source": [
    "step2 = llm.invoke([user_input, step1, (\"human\", \"How many people live there?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMJMoHr1hqj1",
    "outputId": "003563fb-3be4-4add-bf74-d53ee0167caf"
   },
   "outputs": [],
   "source": [
    "print(step2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mvnz8m_DMMmU"
   },
   "source": [
    "# Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gfd9iVl7Jr-u",
    "outputId": "3e3234da-1efb-45bf-dbed-430d7b0ff547"
   },
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\", google_api_key=google_api_key)\n",
    "result = llm.invoke(\"What is the capital of the USA?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3baOgBiJ_J5"
   },
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable\n",
    "def run():\n",
    "  return llm.invoke(\"What is the capital of the USA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyOIhP0sKinK"
   },
   "outputs": [],
   "source": [
    "from langsmith import Client, tracing_context, traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "langsmith_client = Client(\n",
    "  api_key=userdata.get('LANGSMITH_API_KEY'),\n",
    "  api_url=\"https://api.smith.langchain.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCLB7583Kum7"
   },
   "outputs": [],
   "source": [
    "with tracing_context(enabled=True):\n",
    "  result = llm.invoke(\"What is the capital of the USA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nb0XzmgdK-K2"
   },
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is the capital of UK?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKQKlH4Ehs0S"
   },
   "source": [
    "# Using external tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k923Q9sIhvGZ"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGUb7713i7Bm"
   },
   "source": [
    "Let's demonstrate how we can instruct an LLM to use an external tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sh4Wd-G3h4Y1",
    "outputId": "843b014c-5d17-4ce8-f07a-fb671c3ba1c5"
   },
   "outputs": [],
   "source": [
    "task = (\n",
    "    \"In 1990, the average cost of a gallon of gasoline was $1.16. If the \"\n",
    "    \"inflation rate from 1990 to today has been a cumulative 180%, what would \"\n",
    "    \"that gallon of gas cost in today's money? How does that compare to the \"\n",
    "    \"current average price of gas?\"\n",
    ")\n",
    "\n",
    "raw_prompt_template = (\n",
    "  \"You have access to search engine that provides you an \"\n",
    "  \"information about current events. \"\n",
    "  \"Given the question, decide whether you need an additional \"\n",
    "  \"information from the search engine, and if yes, reply with 'SEARCH: \"\n",
    "   \"<generated query>'. Only if you know enough to answer the user \"\n",
    "   \"then reply with 'RESPONSE <final response>').\\n\"\n",
    "   \"Now, act to answer a user question:\\n{question}\"\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(raw_prompt_template)\n",
    "\n",
    "response = (prompt_template | llm).invoke(task)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3REc5mrkpWH"
   },
   "source": [
    "Technical note: a _PromptTemplate_ allows you substitute variables when executing the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07_b8fCHkup9",
    "outputId": "f714fe31-944f-49bc-b072-3cebfd0fe0b8"
   },
   "outputs": [],
   "source": [
    "prompt_template.invoke({\"question\": \"TEST\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SkYzBWspmWN9"
   },
   "outputs": [],
   "source": [
    "query = \"average gas price today\"\n",
    "search_result = \"3.349\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5K4EDs_XlAYu",
    "outputId": "14a2fcb3-0d4c-438d-ebf7-8b9c86cc2be9"
   },
   "outputs": [],
   "source": [
    "raw_prompt_template = (\n",
    "  \"You have access to search engine that provides you an \"\n",
    "  \"information about current events. \"\n",
    "  \"Given the question, decide whether you need an additional \"\n",
    "  \"information from the search engine, and if yes, reply with 'SEARCH: \"\n",
    "   \"<generated query>'. Only if you know enough to answer the user \"\n",
    "   \"then reply with 'RESPONSE <final response>').\\n\"\n",
    "   #\"Today is {date}.\"\n",
    "   \"Now, act to answer a user question and \"\n",
    "   \"take into account your previous actions:\\n\"\n",
    "   \"HUMAN: {question}\\n\"\n",
    "   \"AI: SEARCH: {query}\\n\"\n",
    "   \"RESPONSE FROM SEARCH: {search_result}\\n\"\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(raw_prompt_template)\n",
    "\n",
    "result = (prompt_template | llm).invoke({\"question\": task, \"query\": query, \"search_result\": search_result, \"date\": \"Feb 2025\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDUNa8RKmoIM"
   },
   "source": [
    "# Creating tools with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DY35rHYm9XS"
   },
   "source": [
    "Let's use a DuckDuckGo search through LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z5m-A_wCmpz4",
    "outputId": "bc50fabe-a911-4ce7-95a6-453897ef59b7"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "print(f\"Tool's name = {search.name}\")\n",
    "print(f\"Tool's name = {search.description}\")\n",
    "print(f\"Tool's arg schema = {search.args_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbKbhjk-m8u3",
    "outputId": "66a8cb24-1089-4f53-eea0-94394c98f0ac"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.ddg_search.tool import DDGInput\n",
    "\n",
    "print(DDGInput.model_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWxbW_j4nFP1",
    "outputId": "50a3cb9e-7657-4d95-a723-ca37ddcd46e4"
   },
   "outputs": [],
   "source": [
    "query = \"What is the weather in Munich like tomorrow?\"\n",
    "search_input = DDGInput(query=query)\n",
    "result = search.invoke(search_input.model_dump())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnl266NKnKGd",
    "outputId": "e7a60921-189d-4f30-b968-d55f4bdeced0"
   },
   "outputs": [],
   "source": [
    "isinstance(result, str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvfwWJ0VnnWL"
   },
   "source": [
    "Another example - let's use a web API to instruct an LLM to get the latest information about FX rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKDmxcVlnOBe",
    "outputId": "8a292bba-853d-4c32-fc43-e88e5f43a64c"
   },
   "outputs": [],
   "source": [
    "api_spec = \"\"\"\n",
    "openapi: 3.0.0\n",
    "info:\n",
    "  title: Frankfurter Currency Exchange API\n",
    "  version: v1\n",
    "  description: API for retrieving currency exchange rates. Pay attention to the base currency and change it if needed.\n",
    "\n",
    "servers:\n",
    "  - url: https://api.frankfurter.dev/v1\n",
    "\n",
    "paths:\n",
    "  /v1/{date}:\n",
    "    get:\n",
    "      summary: Get exchange rates for a specific date.\n",
    "      parameters:\n",
    "        - in: path\n",
    "          name: date\n",
    "          schema:\n",
    "            type: string\n",
    "            pattern: '^\\d{4}-\\d{2}-\\d{2}$' # YYYY-MM-DD format\n",
    "          required: true\n",
    "          description: The date for which to retrieve exchange rates.  Use YYYY-MM-DD format.  Example: 2009-01-04\n",
    "        - in: query\n",
    "          name: symbols\n",
    "          schema:\n",
    "            type: string\n",
    "          description: Comma-separated list of currency symbols to retrieve rates for. Example: GBP,USD,EUR\n",
    "\n",
    "  /v1/latest:\n",
    "    get:\n",
    "      summary: Get the latest exchange rates.\n",
    "      parameters:\n",
    "        - in: query\n",
    "          name: symbols\n",
    "          schema:\n",
    "            type: string\n",
    "          description: Comma-separated list of currency symbols to retrieve rates for. Example: CHF,GBP\n",
    "        - in: query\n",
    "          name: base\n",
    "          schema:\n",
    "            type: string\n",
    "          description: The base currency for the exchange rates. If not provided, EUR is used as a base currency. Example: USD\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QG_Z_-23nv4-",
    "outputId": "6a177b55-a6bb-4aac-a106-535eafd533bd"
   },
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits.openapi.toolkit import RequestsToolkit\n",
    "from langchain_community.utilities.requests import TextRequestsWrapper\n",
    "\n",
    "toolkit = RequestsToolkit(\n",
    "    requests_wrapper=TextRequestsWrapper(headers={}),\n",
    "    allow_dangerous_requests=True,\n",
    ")\n",
    "\n",
    "for tool in toolkit.get_tools():\n",
    "  print(tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnEVunSun1em"
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "system_message = (\n",
    "  \"You're given the API spec:\\n{api_spec}\\n\"\n",
    "  \"If possible, use this API if a user asks about foreign exchange rates. \"\n",
    ")\n",
    "\n",
    "agent = create_react_agent(llm, toolkit.get_tools(), prompt=system_message.format(api_spec=api_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jIQriaOoJrf",
    "outputId": "11159c65-77fe-4946-8827-1ba89405755a"
   },
   "outputs": [],
   "source": [
    "query = \"What is the swiss franc to US dollar exchange rate?\"\n",
    "\n",
    "for event in agent.stream({\"messages\": [(\"human\", query)]}, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1Wk4graoWyJ"
   },
   "outputs": [],
   "source": [
    "response = llm.invoke([\n",
    "    (\"system\", system_message.format(api_spec=api_spec)),\n",
    "     (\"human\", \"What is the swiss franc to US dollar exchange rate?\")], tools=toolkit.get_tools())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HcdnCOyXrkL5",
    "outputId": "189d0715-b4c9-410d-be7b-39d762799c0b"
   },
   "outputs": [],
   "source": [
    "tool_calls = response.tool_calls\n",
    "print(tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Ueo6PVMirnCa",
    "outputId": "9ff53f77-cd62-4592-e068-74bdab20693f"
   },
   "outputs": [],
   "source": [
    "toolkit.get_tools()[0].run(tool_calls[0][\"args\"][\"__arg1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c-ys0xRGtyb"
   },
   "source": [
    "# Defining tools with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qi-wbywqGrES"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from langchain_core.tools import tool\n",
    "import numexpr as ne\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculates a single mathematical expression, incl. complex numbers.\n",
    "\n",
    "    Always add * to operations, examples:\n",
    "      73i -> 73*i\n",
    "      7pi**2 -> 7*pi**2\n",
    "    \"\"\"\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "5U61OrHlGqUB",
    "outputId": "f95fd9c1-cc1b-4b69-962a-7064af5d3cc7"
   },
   "outputs": [],
   "source": [
    "calculator.invoke(\"2+2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SL1za8A8JVQj",
    "outputId": "9a7d13ba-f5f8-4073-d552-b487be29da76"
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "assert isinstance(calculator, BaseTool)\n",
    "print(f\"Tool name: {calculator.name}\")\n",
    "print(f\"Tool name: {calculator.description}\")\n",
    "print(f\"Tool schema: {calculator.args_schema.model_json_schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DVjhZ_ZF2Hmp",
    "outputId": "4ac0131c-112a-42fb-b4f5-94868b6693bb"
   },
   "outputs": [],
   "source": [
    "print(calculator.args_schema.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-r-MQ8vJWQD",
    "outputId": "8281346d-1ec4-49ce-8c0d-8d03ce57208d"
   },
   "outputs": [],
   "source": [
    "query = \"How much is 2+3i squared?\"\n",
    "\n",
    "agent = create_react_agent(llm, [calculator])\n",
    "\n",
    "for event in agent.stream({\"messages\": [(\"user\", query)]}, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pn-Rwg_RJpph",
    "outputId": "2f4f72ab-5070-48ef-b221-b844965475e9"
   },
   "outputs": [],
   "source": [
    "question = (\n",
    "    #\"I ate 200g of chicken breast, 150g of broccoli, and 50g of brown rice for dinner. \"\n",
    "    \"I ate 200g of chicken breast for dinner. \"\n",
    "    \"How many total calories did I consume, and what percentage of my recommended daily \"\n",
    "    \"protein intake does this meal provide if my recommended intake is 75g?\"\n",
    ")\n",
    "\n",
    "system_hint = \"Think step-by-step. Always use search tool to get the fresh information about events or public facts that can change over time. Always use calculator tool for math computations.\"\n",
    "\n",
    "agent = create_react_agent(\n",
    "    llm, [calculator, search],\n",
    "    prompt=system_hint)\n",
    "\n",
    "for event in agent.stream({\"messages\": [(\"user\", question)]}, stream_mode=\"updates\"):\n",
    "    for _, event_values in event.items():\n",
    "      for message in event_values[\"messages\"]:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iF7KAc3vNUSa"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
    "from langchain_core.tools import tool, convert_runnable_to_tool\n",
    "\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "calculator_with_retry = RunnableLambda(calculator).with_retry(\n",
    "    wait_exponential_jitter=True,\n",
    "    stop_after_attempt=3,\n",
    ")\n",
    "\n",
    "calculator_tool = convert_runnable_to_tool(\n",
    "    calculator_with_retry,\n",
    "    name=\"calculator\",\n",
    "    description=(\n",
    "        \"Calculates a single mathematical expression, incl. complex numbers.\"\n",
    "        \"'\\nAlways add * to operations, examples:\\n73i -> 73*i\\n\"\n",
    "        \"7pi**2 -> 7*pi**2\"\n",
    "    ),\n",
    "    arg_types={\"expression\": \"str\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7k6xsDKrNXZz",
    "outputId": "0de8e942-1fe4-4dbd-e94e-006f10057652"
   },
   "outputs": [],
   "source": [
    "llm.invoke(\"How much is (2+3i)**2\", tools=[calculator_tool]).tool_calls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "j4HPKpAVNa4S",
    "outputId": "ac134d97-0076-416c-cd1f-9d55c101bb33"
   },
   "outputs": [],
   "source": [
    "calculator_tool.invoke({\"expression\": \"(2+3*i)**2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GaHzFTmQNsS_",
    "outputId": "f82fa735-e367-4d90-c5c0-e9902d4c1dae"
   },
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, [calculator_tool])\n",
    "\n",
    "for event in agent.stream({\"messages\": [(\"user\", \"How much is (2+3i)^2\")]}, stream_mode=\"updates\"):\n",
    "    for _, event_values in event.items():\n",
    "      for message in event_values[\"messages\"]:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGC66SuhNpsL",
    "outputId": "8b75e65b-ffb1-49ee-ae76-c16a38213c33"
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculates a single mathematical expression, incl. complex numbers.\"\"\"\n",
    "    return str(ne.evaluate(expression.strip(), local_dict={}))\n",
    "\n",
    "calculator_tool = StructuredTool.from_function(\n",
    "    func=calculator,\n",
    "    handle_tool_error=True\n",
    ")\n",
    "\n",
    "agent = create_react_agent(llm, [calculator_tool])\n",
    "\n",
    "for event in agent.stream({\"messages\": [(\"user\", \"How much is (2+3i)^2\")]}, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIcYnD7GWVHg"
   },
   "source": [
    "# Controlled generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxRWvU6mWV_J"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Step(BaseModel):\n",
    "    \"\"\"A step that is a part of the plan to solve the task.\"\"\"\n",
    "    step: str = Field(description=\"Description of the step\")\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"A plan to solve the task.\"\"\"\n",
    "    steps: list[Step]\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Prepare a step-by-step plan to solve the given task.\\n\"\n",
    "    \"TASK:\\n{task}\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73ZaKWu058Gh"
   },
   "outputs": [],
   "source": [
    "llm1 = llm.with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KpPpJUS6GG8"
   },
   "outputs": [],
   "source": [
    "substituted_prompt = prompt.invoke(\"How to write a bestseller on Amazon about generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzPWj6jC6PAe",
    "outputId": "533c3809-7d48-44ce-802d-41366f0edf8e"
   },
   "outputs": [],
   "source": [
    "llm1.invoke(substituted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cri0nf3CWoMZ",
    "outputId": "8bb801b0-c263-4a3e-9e6d-ae1a9626eb1f"
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm.with_structured_output(Plan)\n",
    "result = chain.invoke(\"How to write a bestseller on Amazon about generative AI?\")\n",
    "assert isinstance(result, Plan)\n",
    "print(f\"Amount of steps: {len(result.steps)}\")\n",
    "for step in result.steps:\n",
    "  print(step.step)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhsJAMnZ5eX-",
    "outputId": "73add84a-acba-42fd-8aca-865b8f073975"
   },
   "outputs": [],
   "source": [
    "type(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKu6NeQG5bBs",
    "outputId": "0a76882f-8db8-4966-ca4e-1f9d1aeaf892"
   },
   "outputs": [],
   "source": [
    "for step in result.steps:\n",
    "  print(step.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EK3McXSmWrHA",
    "outputId": "8e3a4718-81d9-4fb9-ebda-c26117b88222"
   },
   "outputs": [],
   "source": [
    "Plan.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TndH-iMoXjs-"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-zj59-FX_2h"
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "llm1 = ChatVertexAI(model_name=\"gemini-2.5-pro\", project=\"kuligin-sandbox1\")\n",
    "\n",
    "plan_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"OBJECT\",\n",
    "          \"properties\": {\n",
    "              \"step\": {\"type\": \"STRING\"},\n",
    "          },\n",
    "      },\n",
    "}\n",
    "\n",
    "query = \"How to write a bestseller on Amazon about generative AI?\"\n",
    "result = (prompt | llm1.with_structured_output(schema=plan_schema, method=\"json_mode\")).invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mrVI4p00YLFq",
    "outputId": "6a728fc9-51f3-4046-81a9-0e26f58aa47a"
   },
   "outputs": [],
   "source": [
    "assert(isinstance(result, list))\n",
    "print(f\"Amount of steps: {len(result)}\")\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbN3DZRjX82A",
    "outputId": "6c20918b-3d8e-4e3f-b5f7-3603790387cb"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "llm_json = ChatVertexAI(project=\"kuligin-sandbox1\", model_name=\"gemini-2.5-pro\",\n",
    "                        response_mime_type=\"application/json\",\n",
    "                        response_schema=plan_schema)\n",
    "result = (prompt | llm_json | JsonOutputParser()).invoke(query)\n",
    "assert(isinstance(result, list))\n",
    "print(f\"Amount of steps: {len(result)}\")\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "184WuJ7uXxjn",
    "outputId": "338af12d-1d18-4588-c8eb-079f876d8a51"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "response_schema = {\"type\": \"STRING\", \"enum\": [\"positive\", \"negative\", \"neutral\"]}\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Classify the tone of the following customer's review:\"\n",
    "    \"\\n{review}\\n\"\n",
    ")\n",
    "\n",
    "review = \"I like this movie!\"\n",
    "llm_enum = ChatVertexAI(project=\"kuligin-sandbox1\", model_name=\"gemini-1.5-pro-002\",\n",
    "                        response_mime_type=\"text/x.enum\",\n",
    "                        response_schema=response_schema)\n",
    "result = (prompt | llm_enum | StrOutputParser()).invoke(review)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRu-hfd-WzQI"
   },
   "outputs": [],
   "source": [
    "plan_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"OBJECT\",\n",
    "          \"properties\": {\n",
    "              \"step\": {\"type\": \"STRING\"},\n",
    "          },\n",
    "      },\n",
    "}\n",
    "\n",
    "query = \"How to write a bestseller on Amazon about generative AI?\"\n",
    "result = (prompt | llm.with_structured_output(schema=plan_schema, method=\"json_mode\")).invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6OXoubH0W1bM",
    "outputId": "7d9944a0-8635-441c-d3f4-f95d61c59885"
   },
   "outputs": [],
   "source": [
    "assert(isinstance(result, list))\n",
    "print(f\"Amount of steps: {len(result)}\")\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUb6rsG5XY2q",
    "outputId": "3d9114b5-195c-4b07-8abb-a772129fc0d8"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "llm_json = ChatVertexAI(model_name=\"gemini-2.5-pro\", project=\"kuligin-sandbox1\", response_mime_type=\"application/json\", response_schema=plan_schema)\n",
    "result = (prompt | llm_json | JsonOutputParser()).invoke(query)\n",
    "assert(isinstance(result, list))\n",
    "print(f\"Amount of steps: {len(result)}\")\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CKcJiGdXbfG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaiB_FomQ_RN"
   },
   "source": [
    "# Plan-and-solve agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqH_u8NHRBt1"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: list[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "\n",
    "system_prompt_template = (\n",
    "    \"For the given task, come up with a step by step plan.\\n\"\n",
    "    \"This plan should involve individual tasks, that if executed correctly will \"\n",
    "    \"yield the correct answer. Do not add any superfluous steps.\\n\"\n",
    "    \"The result of the final step should be the final answer. Make sure that each \"\n",
    "    \"step has all the information needed - do not skip steps.\"\n",
    ")\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt_template),\n",
    "     (\"user\", \"Prepare a plan how to solve the following task:\\n{task}\\n\")])\n",
    "\n",
    "planner = planner_prompt | ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\", temperature=1.0, google_api_key=google_api_key\n",
    ").with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0Dh4s8SRNTH"
   },
   "outputs": [],
   "source": [
    "task = \"Write a strategic one-pager of building an AI startup?\"\n",
    "plan = planner.invoke(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IP0yIVK7bJs6",
    "outputId": "166eee29-e902-42cf-afd8-e9c902a9389c"
   },
   "outputs": [],
   "source": [
    "plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baMKWyBbRWaf"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=google_api_key)\n",
    "tools = load_tools(\n",
    "  tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"],\n",
    "  llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TA1NJIb2RR4o",
    "outputId": "11c12897-949c-421e-ddd5-4b51fd9ed05d"
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.managed import IsLastStep\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class StepState(AgentState):\n",
    "  plan: str\n",
    "  step: str\n",
    "  task: str\n",
    "\n",
    "system_prompt = (\n",
    "    \"You're a smart assistant that carefully helps to solve complex tasks.\\n\"\n",
    "    \" Given a general plan to solve a task and a specific step, work on this step. \"\n",
    "    \" Don't assume anything, keep in minds things might change and always try to \"\n",
    "    \"use tools to double-check yourself.\\m\"\n",
    "    \" Use a calculator for mathematical computations, use Search to gather\"\n",
    "    \"for information about common facts, fresh events and news, use Arxiv to get \"\n",
    "    \"ideas on recent research and use Wikipedia for common knowledge.\"\n",
    ")\n",
    "\n",
    "step_template = (\n",
    "    \"Given the task and the plan, try to execute on a specific step of the plan.\\n\"\n",
    "    \"TASK:\\n{task}\\n\\nPLAN:\\n{plan}\\n\\nSTEP TO EXECUTE:\\n{step}\\n\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", step_template),\n",
    "])\n",
    "\n",
    "execution_agent = create_react_agent(model=llm, tools=tools+[calculator_tool], state_schema=StepState, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLJE-s-FSJ32"
   },
   "outputs": [],
   "source": [
    "class PlanState(TypedDict):\n",
    "    task: str\n",
    "    plan: Plan\n",
    "    past_steps: Annotated[list[str], add]\n",
    "    final_response: str\n",
    "\n",
    "\n",
    "def get_current_step(state: PlanState) -> int:\n",
    "  \"\"\"Returns the number of current step to be executed.\"\"\"\n",
    "  return len(state.get(\"past_steps\", []))\n",
    "\n",
    "def get_full_plan(state: PlanState) -> str:\n",
    "  \"\"\"Returns formatted plan with step numbers and past results.\"\"\"\n",
    "  full_plan = []\n",
    "  for i, step in enumerate(state[\"plan\"].steps):\n",
    "    full_step = f\"# {i+1}. Planned step: {step}\\n\"\n",
    "    if i < get_current_step(state):\n",
    "      full_step += f\"Result: {state['past_steps'][i]}\\n\"\n",
    "    full_plan.append(full_step)\n",
    "  return \"\\n\".join(full_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "AZy2UXVXSOsv",
    "outputId": "7eecbfcd-d5ed-4945-fc85-9f7a489f5e89"
   },
   "outputs": [],
   "source": [
    "final_prompt = PromptTemplate.from_template(\n",
    "    \"You're a helpful assistant that has executed on a plan.\"\n",
    "    \"Given the results of the execution, prepare the final response.\\n\"\n",
    "    \"Don't assume anything\\nTASK:\\n{task}\\n\\nPLAN WITH RESUlTS:\\n{plan}\\n\"\n",
    "    \"FINAL RESPONSE:\\n\"\n",
    ")\n",
    "\n",
    "async def _build_initial_plan(state: PlanState) -> PlanState:\n",
    "  plan = await planner.ainvoke(state[\"task\"])\n",
    "  return {\"plan\": plan}\n",
    "\n",
    "async def _run_step(state: PlanState) -> PlanState:\n",
    "  plan = state[\"plan\"]\n",
    "  current_step = get_current_step(state)\n",
    "  step = await execution_agent.ainvoke({\"plan\": get_full_plan(state), \"step\": plan.steps[current_step], \"task\": state[\"task\"]})\n",
    "  return {\"past_steps\": [step[\"messages\"][-1].content]}\n",
    "\n",
    "async def _get_final_response(state: PlanState) -> PlanState:\n",
    "  final_response = await (final_prompt | llm).ainvoke({\"task\": state[\"task\"], \"plan\": get_full_plan(state)})\n",
    "  return {\"final_response\": final_response}\n",
    "\n",
    "\n",
    "def _should_continue(state: PlanState) -> Literal[\"run\", \"response\"]:\n",
    "  if get_current_step(state) < len(state[\"plan\"].steps):\n",
    "    return \"run\"\n",
    "  return \"response\"\n",
    "\n",
    "builder = StateGraph(PlanState)\n",
    "builder.add_node(\"initial_plan\", _build_initial_plan)\n",
    "builder.add_node(\"run\", _run_step)\n",
    "builder.add_node(\"response\", _get_final_response)\n",
    "\n",
    "builder.add_edge(START, \"initial_plan\")\n",
    "builder.add_edge(\"initial_plan\", \"run\")\n",
    "builder.add_conditional_edges(\"run\", _should_continue)\n",
    "builder.add_edge(\"response\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "kG28IHL4D_P0",
    "outputId": "c842f46c-17e5-4189-f66f-20ecc250ea22"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(execution_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5P4NhzcSU9S",
    "outputId": "359f402f-31f1-407f-a4f6-7a96c3da3f5e"
   },
   "outputs": [],
   "source": [
    "task = \"Write a strategic one-pager of building an AI startup\"\n",
    "result = await graph.ainvoke({\"task\": task})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQzhzs9jTAmQ"
   },
   "outputs": [],
   "source": [
    "print(result[\"final_response\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hquWZOUKSYL1"
   },
   "outputs": [],
   "source": [
    "async for output in graph.astream({\"task\": task}, stream_mode=\"updates\"):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B32qL0-kgLvZ"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MeiGwF8wgNFQ",
    "outputId": "8930aa23-5b53-459b-f9cf-456a219d1c70"
   },
   "outputs": [],
   "source": [
    "!pip install langchain_google_vertexai langsmith langchain-google-genai duckduckgo-search langchain-community langgraph arxiv wikipedia ddgs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (sandbox_workshop)",
   "language": "python",
   "name": "sandbox_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
