{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyeCwDW69Zf3"
   },
   "source": [
    "# Demo 1 - RAG Pipeline with Chained Prompt Processing\n",
    "By: [Lior Gazit](https://github.com/LiorGazit).  \n",
    "Repo: [Agents-Over-The-Weekend](https://github.com/PacktPublishing/Agents-Over-The-Weekend/tree/main/Lior_Gazit/workshop_september_2025/)  \n",
    "Running LLMs locally for free: This code leverages [`LLMPop`](https://pypi.org/project/llmpop/) that is dedicated to spinning up local or remote LLMs in a unified and modular syntax.  \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PacktPublishing/Agents-Over-The-Weekend/blob/main/Lior_Gazit/workshop_september_2025/codes_for_Lior_Bootcamp_talk_sept2025_demo1.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a> (pick a GPU Colab session for fastest computing)  \n",
    "\n",
    "```\n",
    "Disclaimer: The content and ideas presented in this notebook are solely those of the author, Lior Gazit, and do not represent the views or intellectual property of the author's employer.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn9qIRLS9Zf7"
   },
   "source": [
    "Installing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMpPyY829Zf9",
    "outputId": "ab8f4223-f9bd-483a-de03-a7becd88e22f"
   },
   "outputs": [],
   "source": [
    "# %pip -q install llmpop\n",
    "# %pip -q install sentence-transformers faiss-cpu langchain langchain_core # tiktoken langsmith langchain_openai\n",
    "# %pip -q install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uU75SUy9ZgA"
   },
   "source": [
    "**Imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/version (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f58959c5990>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "⚠️ Ollama no responde; intentando abrir la app en Windows...\n",
      "HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/version (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f5a2016e6e0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "❌ Ollama sigue sin responder.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import os, time, subprocess, requests\n",
    "\n",
    "OLLAMA_HOST = \"http://127.0.0.1:11434\"\n",
    "os.environ[\"OLLAMA_HOST\"] = \"127.0.0.1:11434\"\n",
    "\n",
    "def ensure_ollama_running():\n",
    "    # 1) ¿ya responde?\n",
    "    try:\n",
    "        r = requests.get(f\"{OLLAMA_HOST}/api/version\", timeout=2)\n",
    "        if r.ok:\n",
    "            print(\"✅ Ollama responde.\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    print(\"⚠️ Ollama no responde; intentando abrir la app en Windows...\")\n",
    "\n",
    "    # 2) Lanza la app de Windows (requiere que esté instalada)\n",
    "    pwsh = r\"/mnt/c/Windows/System32/WindowsPowerShell/v1.0/powershell.exe\"\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [pwsh, \"-Command\", \"Start-Process -WindowStyle Hidden Ollama\"],\n",
    "            check=True\n",
    "        )\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        print(\"❌ No se pudo lanzar la app Ollama:\", e)\n",
    "\n",
    "    # 3) Reintenta\n",
    "    try:\n",
    "        r = requests.get(f\"{OLLAMA_HOST}/api/version\", timeout=5)\n",
    "        if r.ok:\n",
    "            print(\"✅ Ollama iniciado.\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    print(\"❌ Ollama sigue sin responder.\")\n",
    "    return False\n",
    "\n",
    "ensure_ollama_running()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb3_eQNB9ZgC"
   },
   "source": [
    "Setting up the Vector Store and RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542,
     "referenced_widgets": [
      "eabcd6d866714eb289cb356a7f7361c5",
      "3ac014f2afba4fe8851f331dfc4dd4f1",
      "1e8d603694454c95ba8d3cafdd92174f",
      "30dc4f695d01432dbf4d86861aa75c27",
      "50fd12b047ec4a47a2039fda91ac006f",
      "8d7e146167c24e10ada24eb291d2966d",
      "f31434fa70ea4245b956e0b1e746c407",
      "bfd5958246164c1e8c18eb155f608a03",
      "7c2dbf487c424c7da1f994a5ef77b2cf",
      "b35aaac60d2749e2bd56101c3da9c34d",
      "3b62c12f8723412e9b2a8cd92400eac5",
      "c6a24b0bb7214549bee348177d58eef1",
      "679dc92a32924c95851c7f4ec0f4d34f",
      "c4b93218050d43ef8f2437057a830058",
      "376ba920ff8b4f6e9cafe0c38fef5c47",
      "c393781e197c42728aa3661229229254",
      "241c3a1461f24f04b5f09ae94e6e7a11",
      "372df47a87984f96872d6468b615e71c",
      "2f12eb9f441e4edea5db758b098af93c",
      "96a343150b9e42738ce3916a09b47487",
      "5b9722f1eccf451ba9a338c76f343275",
      "e014dee625e348ff85e5b967307ab82c",
      "541c2acd28a4473a8d1b2031120d794a",
      "c24545cf1fd54d3aaeaeb580d013278e",
      "51713fa9b3904a4fa058032788f2512b",
      "c98bb13384434d67aa45d7be226bb710",
      "8d66b77aea014870bd4d1e15366898c1",
      "98fe32c8931f4b609be009a96536a796",
      "c1cf03f3b41746988fec3550d58a7bb8",
      "973e8b2cdf0a4ac9b980f04892d56fd6",
      "224d3e7016534778a906fb05fb28e278",
      "d89e4f09adf84385a9cd0485e737c7ee",
      "cd28d3c6f60142e896651db93a768d6e",
      "06f26edf405c4c5f9d309df20e16668d",
      "8b06ab20ff444f09a50edec10edc7264",
      "c6b7b5d2c49f4c80b93980b95e1d4304",
      "fb849d166a4d4852afcd3f69a451202c",
      "937c5307772d4281bf132909d9e8c1d7",
      "43b61cc8e8b04f408ad357501ff07cb5",
      "fea0764fcb8b4b3ca9889f12b8426752",
      "72edd6a369884e2f9f6ae3a517ad9591",
      "3a6d7541ebb643c3bd082c2ec1ee7cc5",
      "e68102f06c994fc389fe6f9d67d1fac9",
      "50743a3618514376bc847bbdba01c262",
      "7517f8380041412e91ffa5651b5e163d",
      "d2199d8373d5476096c1577e851df247",
      "ab7c19c4021f4c63be803c033e334c5a",
      "01b17d54534e4b9d89c05b53c0c5f15b",
      "3ababe52b68a47feb25129d302adfed3",
      "f50af36d75d344c3996f5bc3de67ab11",
      "67766ed18e6a42e6b623ebc0bfca37b6",
      "1643e9659bfb41a4893c121bd8e4fbea",
      "535a2b58c2924b35b7273d5d01c17df4",
      "0ba4cacd18e54b789e15ea3d17f3d5e8",
      "905a3c3a51464681aaec0d3ad0847bcd",
      "becf05d54a49423ebd18864a7b6730db",
      "36ed05cc00c64559a97b2e4ceaae29ac",
      "aa953361e1f74fedb282b67506e92c73",
      "cc9718f332824e2eaa46a41acf92a178",
      "a74fa1a6665449dd9ec41b5c57651c47",
      "e3304cf7066143f3bc832d4ac6d1d6a4",
      "ffa3df92f6e14a338d3357ad6999e5ac",
      "bfa4fd7f13ab476d9d6aa5654d562376",
      "7dbbadfc203647a1bbddc8a0c80be985",
      "72219362144e43a0802ff7c6074a83f7",
      "787e01a757334147921aeaa0f4c9e1b3",
      "b9a67dd97290431697c76d4e69ef7087",
      "8f1187ae4cf34181b4a492dabd9dae5e",
      "4fc515da83a048d28bf8bd112fc0de73",
      "bfd8572c0bb1469eba930f68d9ad3fca",
      "c154b5771cfe4331919973938b4b184f",
      "4e481e24b2e14a63ace1511867b122af",
      "07a9600a6b844c879ae284db2dee979b",
      "a468a1b3e11e4bf4892dc03a30367936",
      "d904bf0f39c34cc4b4cc347f8cb3633e",
      "81e7717c89eb40fb9a37fd76d0cfe5b7",
      "70750c5f598f4e4db1632d972af8e622",
      "43969edb2e8c4f2487d62915097be66f",
      "336f27ce62e54524ba752375dfb08085",
      "1e89fa14ff474fd9a0233ecfc76653be",
      "1c858329484e41fe8e8f3b0232a7516c",
      "a9a7cc60a0f943dfa055cb385f909789",
      "4306293aea084c8996625c7b68f7fa90",
      "bf902a92fc0b43fcae6a2733454d4624",
      "cde0ebff24ac4fa1920cf386b3b8fe51",
      "196f9c18da144d06937e1d68abe90356",
      "6cfb911d7a7a439ab27034053e63d376",
      "3c24b54244c04c78aab238f1af444235",
      "c2d9d84a987a4e6a9e029b5abde976b6",
      "05e591c486a2442dbf626bcba02b0d7f",
      "8ed0ce40cb2f4a12acd3675b71385ec5",
      "6681dfc3a8d64ecdaec3658c9bca6aa3",
      "6cc5df914e2d402599e85c9d7d684995",
      "d3d0036fc7c0405384d509589e6771e0",
      "b118456f19e44ebeba0a88121727be5e",
      "a4927b00501c4b4fb56a1c9d028c7f7c",
      "884ea63faedc4f9c95108434f41464c9",
      "84413e6d07df4e858e4e3befa7a06e58",
      "761b10470f00410f86c3507df8c2de6f",
      "23794be5133e4a088b95f990087e250b",
      "4a221b9a2d0848589ae6c9feae0b445c",
      "65bd9c37b089424897cb7bfac2be7aae",
      "4630d19977f64a58a54925f1242bc8b2",
      "e5ba765424db492cb96660f5d7be8a27",
      "b8e78c3e3ff74dc1ba091f7482430092",
      "6bdfb081c97648a5bf552609d5c963ec",
      "da2fa3a791b14534a05689853c7a06c0",
      "9c280ad675ef4f0780dad62d1604f0d4",
      "5019c200affb42649d73b609581ad2f3",
      "627e7c2c343d4d8a926ef66794460369",
      "901c1a19ec6d485bbfefb15334294213",
      "68b894a0e1a74d899ad11408180d5055",
      "9361af0f4c7242afb058269aacf07a31",
      "1b2696dbc306460eabd43f63d59ed14c",
      "507ce82ebb464df8852d54fd822e057c",
      "99fd12da400d4bc0899e2a1dfad713b0",
      "716d9560eea34dd5b71afabb692c7fa1",
      "f16c1efc3a71447586e22570708902a3",
      "a1ef4ac445ea496b828d0631af92f65f",
      "94aaf6da48d946d3b84010f4c0233db5",
      "0440215b08d14a55acd5071a7dd808aa"
     ]
    },
    "id": "EEqyCnZQ9ZgD",
    "outputId": "93d703eb-6262-439d-cefe-f7a4e8098a49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Retrieved documents for context:\n",
      " [\"The company's earnings call mentioned concerns over increased production costs.\", 'Recent financial filings show revenue growth despite supply chain issues.']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Example documents (could be clinical notes, financial filings, etc.)\n",
    "documents = [\n",
    "    \"Patient has diabetes type 2 and shows high glucose levels.\",\n",
    "    \"Recent financial filings show revenue growth despite supply chain issues.\",\n",
    "    \"Patient diagnosed with hypertension, recommended lifestyle changes.\",\n",
    "    \"The company's earnings call mentioned concerns over increased production costs.\",\n",
    "]\n",
    "\n",
    "# Step 1: Create embeddings using SentenceTransformer\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # lightweight embedding model\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = embedding_model.encode(documents)\n",
    "\n",
    "# Step 2: Setup FAISS vector store\n",
    "dimension = document_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(document_embeddings)\n",
    "\n",
    "# Step 3: Define the retriever(query) function\n",
    "def retriever(query, top_k=2):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    # Perform the similarity search in the FAISS index\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "\n",
    "    # Retrieve the top_k most similar documents\n",
    "    retrieved_docs = [documents[idx] for idx in indices[0]]\n",
    "\n",
    "    return retrieved_docs\n",
    "\n",
    "# Example usage of the retriever\n",
    "query = \"What did the company say about production costs?\"\n",
    "context_docs = retriever(query)\n",
    "print(\"\\n\\nRetrieved documents for context:\\n\", context_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpsLTVeT9ZgE"
   },
   "source": [
    "Prompting using a locally hosted LLM via Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hdYXyYZ9ZgF",
    "outputId": "42822353-3d21-4dd4-e168-1182e0996ae6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Ollama server...\n",
      "→ Ollama PID: 497\n",
      "⏳ Waiting for Ollama to be ready…\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Ollama server at http://127.0.0.1:11434 failed to start within 15s.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m context_docs \u001b[38;5;241m=\u001b[39m retriever(query)\n\u001b[1;32m      6\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the following context, answer the question:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontext_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m local_llm \u001b[38;5;241m=\u001b[39m \u001b[43minit_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3.2:1b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mollama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m((prompt \u001b[38;5;241m|\u001b[39m local_llm)\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m:question})\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/mnt/d/sandbox_workshop/.venv/lib/python3.10/site-packages/llmpop/init_llm.py:153\u001b[0m, in \u001b[0;36minit_llm\u001b[0;34m(model, provider, provider_kwargs, verbose, **chat_init_kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⏳ Waiting for Ollama to be ready…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m \u001b[43mwait_for_ollama_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReady!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/d/sandbox_workshop/.venv/lib/python3.10/site-packages/llmpop/init_llm.py:24\u001b[0m, in \u001b[0;36mwait_for_ollama_ready\u001b[0;34m(host, port, timeout)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError:\n\u001b[1;32m     23\u001b[0m         sleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama server at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed to start within \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Ollama server at http://127.0.0.1:11434 failed to start within 15s."
     ]
    }
   ],
   "source": [
    "from llmpop import init_llm\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "query = \"What is the patient's diagnosis given these notes?\"\n",
    "context_docs = retriever(query)\n",
    "question = f\"Using the following context, answer the question:\\n\\n{context_docs}\\n\\nQ: {query}\\n\\n---\\nA:\"\n",
    "local_llm = init_llm(model=\"llama3.2:1b\", provider=\"ollama\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Q: {question}\\nA:\")\n",
    "print((prompt | local_llm).invoke({\"question\":question}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gK1oYpXy9ZgG"
   },
   "source": [
    "Prompting using OpenAI's API (paid) route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xv5oEytT9ZgG",
    "outputId": "d4953a77-0c9b-436d-9a90-426a93486598"
   },
   "outputs": [],
   "source": [
    "# In Colab, use getpass to securely prompt for your API key\n",
    "from getpass import getpass\n",
    "import openai\n",
    "\n",
    "openai.api_key = getpass(\"Paste your OpenAI API key: \")\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":\"You are a medical assistant.\"},\n",
    "        {\"role\":\"user\",  \"content\": question}\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_api = response.choices[0].message.content\n",
    "print(\"\\n\\n\")\n",
    "print(answer_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (sandbox_workshop)",
   "language": "python",
   "name": "sandbox_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
